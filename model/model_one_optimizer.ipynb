{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392855cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages for the whole script\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Independent, kl_divergence\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "import gymnasium as gym39\n",
    "import mujoco\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import minari\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "from skill_model import SkillPolicy, SkillPosterior, SkillPrior, TAWM, MoGSkillPrior\n",
    "from utils import save_checkpoint\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the paper, each layer contains 256 neurons\n",
    "NUM_NEURONS = 256\n",
    "# The dimension of the abstract skill variable, z\n",
    "Z_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "dict_keys(['achieved_goal', 'desired_goal', 'observation'])\n",
      "(1001, 27)\n",
      "(1001, 2)\n"
     ]
    }
   ],
   "source": [
    "# Loads the AntMaze dataset in Minari format\n",
    "ant_maze_dataset = minari.load_dataset('D4RL/antmaze/medium-diverse-v1')\n",
    "\n",
    "print(ant_maze_dataset[0].actions.shape)\n",
    "print(ant_maze_dataset[0].observations.keys())\n",
    "print(ant_maze_dataset[0].observations[\"observation\"].shape)\n",
    "print(ant_maze_dataset[0].observations[\"achieved_goal\"].shape)\n",
    "\n",
    "# B, the number of subtrajectories per batch (from paper)\n",
    "B = 100\n",
    "\n",
    "# T, the length of each subtrajectory (from paper)\n",
    "T = 40\n",
    "\n",
    "# AntMaze state and action dims (from Minari)\n",
    "state_dim = 31\n",
    "action_dim = 8\n",
    "\n",
    "# Initialize the models\n",
    "q_phi = SkillPosterior(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "pi_theta = SkillPolicy(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "p_psi = TAWM(state_dim=state_dim).to(device)\n",
    "# p_omega = SkillPrior(state_dim=state_dim).to(device)\n",
    "p_omega = MoGSkillPrior(state_dim=state_dim, K=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89929230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_episode_splits(minari_dataset, train=0.8, val=0.1, test=0.1, seed=0):\n",
    "    \"\"\"Return three lists of episode indices (train_ids, val_ids, test_ids).\"\"\"\n",
    "    # Materialize all episodes once so we know how many there are\n",
    "    episodes = list(minari_dataset.iterate_episodes())\n",
    "    n = len(episodes)\n",
    "    idxs = list(range(n))\n",
    "    # Shuffle the indices\n",
    "    random.Random(seed).shuffle(idxs)\n",
    "    n_train = int(round(train * n))\n",
    "    n_val = int(round(val * n))\n",
    "    train_ids = idxs[:n_train]\n",
    "    val_ids = idxs[n_train:n_train+n_val]\n",
    "    test_ids = idxs[n_train+n_val:]\n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "class SubtrajDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loops over minari_dataset.iterate_episodes(), but keeps only episodes whose index is in episode_ids\n",
    "    \"\"\"\n",
    "    def __init__(self, minari_dataset, T, episode_ids, stride=3):\n",
    "        self.T = T\n",
    "        self.items = []  \n",
    "\n",
    "        # Iterate all episodes but only process those whose global index is in episode_ids\n",
    "        for ep_idx, ep in enumerate(minari_dataset.iterate_episodes()):\n",
    "            if ep_idx not in set(episode_ids):\n",
    "                continue\n",
    "            obs = ep.observations[\"observation\"]          \n",
    "            ach = ep.observations[\"achieved_goal\"]        \n",
    "            act = ep.actions                               \n",
    "            Ltot = len(obs)\n",
    "            if Ltot < T + 1:\n",
    "                continue\n",
    "\n",
    "            state_ext = np.concatenate([obs, ach], axis=-1).astype(np.float32)\n",
    "            for t in range(0, Ltot - T, stride):\n",
    "                state_seq = state_ext[t:t+T]         \n",
    "                s0 = state_seq[0]             \n",
    "                action_seq = act[t:t+T].astype(np.float32)  \n",
    "                sT = state_ext[t+T]           \n",
    "                self.items.append((s0, state_seq, action_seq, sT))\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"standardize s0, state_sequence, and sT by (x - mean) / std\"\"\"\n",
    "        \n",
    "        s0, S, A, sT = self.items[i]\n",
    "        if hasattr(self, \"stats\") and self.stats is not None:\n",
    "            S_mean, S_std = self.stats\n",
    "            S  = (S  - S_mean) / S_std\n",
    "            s0 = (s0 - S_mean) / S_std\n",
    "            sT = (sT - S_mean) / S_std\n",
    "            A  = A\n",
    "        return {\n",
    "            \"s0\": torch.as_tensor(s0, dtype=torch.float32),\n",
    "            \"state_sequence\": torch.as_tensor(S, dtype=torch.float32),\n",
    "            \"action_sequence\": torch.as_tensor(A, dtype=torch.float32),\n",
    "            \"sT\": torch.as_tensor(sT, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "def collate(batch):\n",
    "    return {\n",
    "        \"s0\": torch.stack([b[\"s0\"] for b in batch], 0),\n",
    "        \"state_sequence\": torch.stack([b[\"state_sequence\"] for b in batch], 0),\n",
    "        \"action_sequence\": torch.stack([b[\"action_sequence\"] for b in batch], 0),\n",
    "        \"sT\": torch.stack([b[\"sT\"] for b in batch], 0),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:800  val:0  test:200\n",
      "train:768800  val:0  test:192200\n"
     ]
    }
   ],
   "source": [
    "# Pick indices for train/test/split\n",
    "train_ids, val_ids, test_ids = make_episode_splits(ant_maze_dataset, train=0.8, val=0.0, test=0.2, seed=0)\n",
    "print(f\"train:{len(train_ids)}  val:{len(val_ids)}  test:{len(test_ids)}\")\n",
    "\n",
    "# Datasets from episode subsets\n",
    "train_ds = SubtrajDataset(ant_maze_dataset, T=T, episode_ids=train_ids, stride=3)\n",
    "val_ds = SubtrajDataset(ant_maze_dataset, T=T, episode_ids=val_ids,   stride=3)\n",
    "test_ds = SubtrajDataset(ant_maze_dataset, T=T, episode_ids=test_ids,  stride=3)  \n",
    "\n",
    "print(f\"train:{len(train_ds)}  val:{len(val_ds)}  test:{len(test_ds)}\")\n",
    "\n",
    "# find per-feature mean and std from all state_sequence timesteps in train_ds\n",
    "def compute_stats(ds):\n",
    "    Ss = []\n",
    "    for item in ds.items:\n",
    "        Ss.append(item[1])  # state_sequence [T,29]\n",
    "    S = np.concatenate([x.reshape(-1, x.shape[-1]) for x in Ss], axis=0)\n",
    "    S_mean, S_std = S.mean(0), S.std(0) + 1e-6\n",
    "    return (S_mean, S_std)\n",
    "\n",
    "S_mean, S_std = 0, 1\n",
    "\n",
    "# pass stats into datasets\n",
    "train_ds.stats = (S_mean, S_std)\n",
    "val_ds.stats = (S_mean, S_std)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=B, shuffle=True,  collate_fn=collate, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=B, shuffle=False, collate_fn=collate, drop_last=False)\n",
    "\n",
    "test_ds.stats = (S_mean, S_std)\n",
    "test_loader = DataLoader(test_ds, batch_size=B, shuffle=False, collate_fn=collate, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = 1.0, 1.0  \n",
    "\n",
    "def compute_loss(batch):\n",
    "    s0, S, A, sT = batch[\"s0\"], batch[\"state_sequence\"], batch[\"action_sequence\"], batch[\"sT\"]\n",
    "    B, T, _  = S.shape\n",
    "    denom = B * T\n",
    "\n",
    "    # State encoder\n",
    "    mu_q, std_q = q_phi(S, A)                      \n",
    "    z = mu_q + std_q * torch.randn_like(mu_q)\n",
    "\n",
    "    # Low-level policy pi_theta(a|s,z)\n",
    "    z_bt = z.unsqueeze(1).expand(B, T, -1)         \n",
    "    mu_pi, std_pi = pi_theta(S.reshape(B*T, -1), z_bt.reshape(B*T, -1))\n",
    "    mu_pi, std_pi = mu_pi.view(B, T, -1), std_pi.view(B, T, -1)\n",
    "    a_dist  = Independent(Normal(mu_pi, std_pi), 1)        \n",
    "\n",
    "    # Compute policy loss\n",
    "    a_loss  = -a_dist.log_prob(A).sum() / denom\n",
    "    \n",
    "    mu_pr, std_pr = p_omega(s0)                              \n",
    "    prior_dist = Independent(Normal(mu_pr, std_pr), 1)\n",
    "    log_prior = prior_dist.log_prob(z).sum() / denom\n",
    "    post_dist = Independent(Normal(mu_q,  std_q),  1)\n",
    "    log_post  = post_dist.log_prob(z).sum() / denom\n",
    "\n",
    "    # w/0 KL balancing\n",
    "    kl_loss = - log_prior + log_post\n",
    "\n",
    "    # Detach gradient\n",
    "    z_detached = z.detach()\n",
    "\n",
    "    # TAWM over terminal state\n",
    "    mu_T, std_T = p_psi(s0, z_detached)                            \n",
    "    sT_dist = Independent(Normal(mu_T, std_T), 1)\n",
    "\n",
    "    # State decoder loss\n",
    "    sT_loss = -sT_dist.log_prob(sT).sum() / denom\n",
    "\n",
    "    # Overall loss (naive VI loss from paper)\n",
    "    loss = alpha * sT_loss + a_loss + beta * kl_loss\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"policy_loss\": a_loss,\n",
    "        \"kl_loss\": kl_loss,\n",
    "        \"state_decoder_loss\": sT_loss\n",
    "    }\n",
    "\n",
    "alpha, beta = 1.0, 1.0  \n",
    "\n",
    "def compute_loss_klbalancing(batch):\n",
    "    s0, S, A, sT = batch[\"s0\"], batch[\"state_sequence\"], batch[\"action_sequence\"], batch[\"sT\"]\n",
    "    B, T, _  = S.shape\n",
    "    denom = B * T\n",
    "\n",
    "    # State encoder\n",
    "    mu_q, std_q = q_phi(S, A)                      \n",
    "    z = mu_q + std_q * torch.randn_like(mu_q)\n",
    "\n",
    "    # Low-level policy pi_theta(a|s,z)\n",
    "    z_bt = z.unsqueeze(1).expand(B, T, -1)         \n",
    "    mu_pi, std_pi = pi_theta(S.reshape(B*T, -1), z_bt.reshape(B*T, -1))\n",
    "    mu_pi, std_pi = mu_pi.view(B, T, -1), std_pi.view(B, T, -1)\n",
    "    a_dist  = Independent(Normal(mu_pi, std_pi), 1)        \n",
    "\n",
    "    # Compute policy loss\n",
    "    a_loss  = -a_dist.log_prob(A).sum() / denom\n",
    "\n",
    "    log_prior = p_omega.log_prob(z, s0).sum() / denom                              \n",
    "    post_dist = Independent(Normal(mu_q,  std_q),  1)\n",
    "    log_post  = post_dist.log_prob(z)\n",
    "\n",
    "    kl_post = (log_post - log_prior.detach())\n",
    "\n",
    "    # Detach gradient\n",
    "    z_detached = z.detach()\n",
    "\n",
    "    # w/ KL balancinga\n",
    "    alpha_balancing = 0.8\n",
    "    log_prior_det = p_omega.log_prob(z_detached, s0)\n",
    "    kl_prior = (log_post.detach() - log_prior_det)\n",
    "    kl_loss = (alpha_balancing * kl_post + (1 - alpha_balancing) * kl_prior).mean()\n",
    "\n",
    "\n",
    "    # TAWM over terminal state\n",
    "    mu_T, std_T = p_psi(s0, z_detached)                            \n",
    "    sT_dist = Independent(Normal(mu_T, std_T), 1)\n",
    "\n",
    "    # State decoder loss\n",
    "    sT_loss = -sT_dist.log_prob(sT).sum() / denom\n",
    "\n",
    "    # Overall loss (naive VI loss from paper)\n",
    "    loss = alpha * sT_loss + a_loss + beta * kl_loss\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"policy_loss\": a_loss,\n",
    "        \"kl_loss\": kl_loss,\n",
    "        \"state_decoder_loss\": sT_loss\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(val_loader, q_phi, pi_theta, p_psi, p_omega, device):\n",
    "    \"\"\"Compute validation loss\"\"\"\n",
    "    q_phi.eval()\n",
    "    pi_theta.eval()\n",
    "    p_psi.eval()\n",
    "    p_omega.eval()\n",
    "    loss_sum,policy_loss_sum, kl_loss_sum, state_decoder_loss_sum, n = 0.0, 0.0, 0.0, 0.0, 0\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        terms = compute_loss_klbalancing(batch)\n",
    "        loss = terms[\"loss\"]\n",
    "        policy_loss = terms[\"policy_loss\"]\n",
    "        kl_loss = terms[\"kl_loss\"]\n",
    "        state_decoder_loss = terms[\"state_decoder_loss\"]\n",
    "        loss_sum += float(loss.item())\n",
    "        policy_loss_sum += float(policy_loss.item())\n",
    "        kl_loss_sum += float(kl_loss.item())\n",
    "        state_decoder_loss_sum += float(state_decoder_loss.item())\n",
    "\n",
    "        n += 1\n",
    "    if n == 0: \n",
    "        return None, None, None, None\n",
    "    return loss_sum / n, policy_loss_sum / n, kl_loss_sum / n, state_decoder_loss_sum / n\n",
    "\n",
    "def skill_model_training_with_val(\n",
    "    train_loader, val_loader,\n",
    "    q_phi, pi_theta, p_psi, p_omega,\n",
    "    lr=5e-5,\n",
    "    epochs=50, steps=1, grad_clip=1.0\n",
    "):\n",
    "    q_phi.to(device)\n",
    "    pi_theta.to(device)\n",
    "    p_psi.to(device)\n",
    "    p_omega.to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(list(q_phi.parameters()) + list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), lr=lr)\n",
    "\n",
    "    tr, va = [], []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        q_phi.train()\n",
    "        pi_theta.train()\n",
    "        p_psi.train()\n",
    "        p_omega.train()\n",
    "        loss_run, policy_loss_run, kl_loss_run, state_decoder_loss_run = 0.0, 0.0, 0.0, 0.0 # Running loss in current epoch\n",
    "\n",
    "        nb = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Rebuilds dictionary but moves tensors to the device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            nb += 1\n",
    "\n",
    "            for _ in range(steps):\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                terms = compute_loss_klbalancing(batch)\n",
    "                loss = terms[\"loss\"]\n",
    "                policy_loss = terms[\"policy_loss\"]\n",
    "                kl_loss = terms[\"kl_loss\"]\n",
    "                state_decoder_loss = terms[\"state_decoder_loss\"]\n",
    "                loss.backward()\n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(list(q_phi.parameters()) + list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), grad_clip)\n",
    "                opt.step()\n",
    "            loss_run += float(loss.item())\n",
    "            policy_loss_run += float(policy_loss.item())\n",
    "            kl_loss_run += float(kl_loss.item())\n",
    "            state_decoder_loss_run += float(state_decoder_loss.item())\n",
    "\n",
    "\n",
    "        # Calculate the average losses over all the batches in the epoch\n",
    "        loss_epoch = loss_run / max(1, nb)\n",
    "        policy_loss_epoch = policy_loss_run / max(1, nb)\n",
    "        kl_loss_epoch = kl_loss_run / max(1, nb)\n",
    "        state_decoder_loss_epoch = state_decoder_loss_run / max(1, nb)\n",
    "\n",
    "        tr.append(loss_epoch)\n",
    "\n",
    "        # validation\n",
    "        v_loss, v_policy_loss, v_kl_loss, v_state_decoder_loss = eval_epoch(val_loader, q_phi, pi_theta, p_psi, p_omega, device)\n",
    "        va.append(v_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch:03d}/{epochs}] \"\n",
    "              f\"train loss:{loss_epoch:.4f} \"\n",
    "              f\"| val loss:{v_loss:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"train/loss\": loss_epoch,\n",
    "            \"train/policy_loss\": policy_loss_epoch,\n",
    "            \"train/kl_loss\": kl_loss_epoch,\n",
    "            \"train/state_decoder_loss\": state_decoder_loss_epoch,\n",
    "            \"val/loss\": v_loss,\n",
    "            \"val/policy_loss\": v_policy_loss,\n",
    "            \"val/kl_loss\": v_kl_loss,\n",
    "            \"val/state_decoder_loss\": v_state_decoder_loss,\n",
    "            \"epoch\": epoch\n",
    "        }, step=epoch)\n",
    "\n",
    "    plt.figure(figsize=(7.5,4.5))\n",
    "    plt.plot(tr, label=\"Train loss\")\n",
    "    if all(v is not None for v in va):\n",
    "        plt.plot(va, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(\"EM training: train vs. val losses\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    wandb.log({\"plots/loss_curves\": wandb.Image(fig)}, step=epoch)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return {\"train_loss\": tr, \"val_E\": va}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9163f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwilliam-huang-08\u001b[0m (\u001b[33mwilliam-huang-08-yale-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/williamhuang/Documents/Yale/Research/CMUBiorobotics/OPOSM/wandb/run-20251124_170647-3xcm1bq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning/runs/3xcm1bq3' target=\"_blank\">antmaze-medium_em</a></strong> to <a href='https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning' target=\"_blank\">https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning/runs/3xcm1bq3' target=\"_blank\">https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning/runs/3xcm1bq3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"tawm-skill-learning\",\n",
    "    name=\"antmaze-medium_em\",\n",
    "    config=dict(\n",
    "        B=B, T=T, Z_DIM=Z_DIM, NUM_NEURONS=NUM_NEURONS,\n",
    "        e_lr=5e-5, m_lr=5e-5, e_steps=1, m_steps=1,\n",
    "        dataset=\"D4RL/antmaze/medium-diverse-v1\",\n",
    "        device=device\n",
    "    )\n",
    ")\n",
    "\n",
    "wandb.watch([q_phi, pi_theta, p_psi, p_omega], log=\"gradients\", log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "curves = skill_model_training_with_val(train_loader, test_loader, q_phi, pi_theta, p_psi, p_omega, epochs=100, e_lr=5e-5, m_lr=5e-5, e_steps=1, m_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d5f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ee84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(\"checkpoints/antmaze_diverse_em_250_1.pth\", q_phi, pi_theta, p_psi, p_omega, B, T, Z_DIM, NUM_NEURONS, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oposm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
