{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392855cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages for the whole script\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Independent, TransformedDistribution\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "import gymnasium as gym39\n",
    "import mujoco\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import minari\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf1ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the paper, each layer contains 256 neurons\n",
    "NUM_NEURONS = 256\n",
    "# The dimension of the abstract skill variable, z\n",
    "Z_DIM = 256\n",
    "\n",
    "# Skill Posterior, q_phi\n",
    "class SkillPosterior(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, h_dim=NUM_NEURONS, n_gru_layers=4):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.state_emb = nn.Sequential(\n",
    "            nn.Linear(state_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.bi_gru = nn.GRU(\n",
    "            input_size=h_dim + action_dim,\n",
    "            hidden_size=h_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            num_layers=n_gru_layers\n",
    "        )\n",
    "\n",
    "        self.mean = MeanNetwork(in_dim=2*h_dim, out_dim=Z_DIM)\n",
    "        self.std  = StandardDeviationNetwork(in_dim=2*h_dim, out_dim=Z_DIM)\n",
    "\n",
    "\n",
    "    def forward(self, state_sequence, action_sequence):\n",
    "        # state_sequence: [B, T, state_dim]\n",
    "        s_emb = self.state_emb(state_sequence)                 \n",
    "        x_in  = torch.cat([s_emb, action_sequence], dim=-1)   \n",
    "        feats, _ = self.bi_gru(x_in)                          \n",
    "        seq_emb = feats[:, -1, :] # *** use last time step, not mean ***\n",
    "        mean = self.mean(seq_emb)\n",
    "        std  = self.std(seq_emb)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "# Low-Level Skill-Conditioned Policy, pi_theta\n",
    "class SkillPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, h_dim=NUM_NEURONS, a_dist='normal', max_sig=None, fixed_sig=None):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.a_dist = a_dist\n",
    "        self.max_sig = max_sig\n",
    "        self.fixed_sig = fixed_sig\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_dim + Z_DIM, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, action_dim)\n",
    "        )\n",
    "        self.sig_head  = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, z):\n",
    "        # state: [B*T, state_dim], z: [B*T, Z_DIM]\n",
    "        x = torch.cat([state, z], dim=-1)\n",
    "        feats = self.layers(x)\n",
    "        mean  = self.mean_head(feats)\n",
    "        if self.max_sig is None:\n",
    "            sig = F.softplus(self.sig_head(feats))\n",
    "        else:\n",
    "            sig = self.max_sig * torch.sigmoid(self.sig_head(feats))\n",
    "        if self.fixed_sig is not None:\n",
    "            sig = self.fixed_sig * torch.ones_like(sig)\n",
    "        return mean, sig\n",
    "\n",
    "        \n",
    "\n",
    "# Temporally-Abstract World Model, p_psi\n",
    "class TAWM(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: initial state, along with the abstract skill\n",
    "    Output: mean and std over terminal state\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for initial state and abstract skill (concatenated)\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, h_dim=NUM_NEURONS, per_element_sigma=True):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.per_element_sigma = per_element_sigma\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_dim + Z_DIM, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, state_dim)\n",
    "        )\n",
    "        if per_element_sigma:\n",
    "            self.sig_head = nn.Sequential(\n",
    "                nn.Linear(h_dim, h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(h_dim, state_dim),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "        else:\n",
    "            self.sig_head = nn.Sequential(\n",
    "                nn.Linear(h_dim, h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(h_dim, 1),\n",
    "                nn.Softplus()\n",
    "            )\n",
    "\n",
    "    def forward(self, s0, z):\n",
    "        # s0: [B, state_dim], z: [B, Z_DIM]\n",
    "        x = torch.cat([s0, z], dim=-1)\n",
    "        feats = self.layers(x)\n",
    "        mean  = self.mean_head(feats)\n",
    "        sig   = self.sig_head(feats)\n",
    "        if not self.per_element_sigma:\n",
    "            sig = sig.expand(-1, self.state_dim)\n",
    "        return mean, sig\n",
    "\n",
    "\n",
    "# Skill Prior, p_omega\n",
    "class SkillPrior(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: Initial state, s0, in the trajectory\n",
    "    Output: mean and std over the abstract skill, z\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for the initial state\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, h_dim=NUM_NEURONS):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, Z_DIM)\n",
    "        )\n",
    "        self.sig_head = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, Z_DIM),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, s0):\n",
    "        feats = self.layers(s0)\n",
    "        mean = self.mean_head(feats)\n",
    "        std  = self.sig_head(feats)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "class MeanNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: tensor to calculate mean\n",
    "    Output: mean of input w/ dimension out_dim\n",
    "\n",
    "    1. 2-layer network w/ ReLU activation for the first layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_dim, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class StandardDeviationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: tensor to calculate std\n",
    "    Output: std of input w/ dimension out_dim\n",
    "\n",
    "    Note: the standard deviation is lower and upper bounded at 0.05 and 2.0\n",
    "    - if std is 0, then log(std) -> inf\n",
    "    - if std is large, then can affect training\n",
    "\n",
    "    1. 2-layer linear network with ReLU activation after first layer and softplus after second\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, min_std=0.05, max_std=5.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.min_std = min_std\n",
    "        self.max_std = max_std\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        std = self.softplus(x) \n",
    "        #+ self.min_std  # lower bound\n",
    "        #std = torch.clamp(std, max=self.max_std)\n",
    "        return std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "dict_keys(['achieved_goal', 'desired_goal', 'observation'])\n",
      "(1001, 27)\n",
      "(1001, 2)\n"
     ]
    }
   ],
   "source": [
    "# Loads the AntMaze dataset in Minari format\n",
    "ant_maze_dataset = minari.load_dataset('D4RL/antmaze/medium-diverse-v1')\n",
    "\n",
    "print(ant_maze_dataset[0].actions.shape)\n",
    "print(ant_maze_dataset[0].observations.keys())\n",
    "print(ant_maze_dataset[0].observations[\"observation\"].shape)\n",
    "print(ant_maze_dataset[0].observations[\"achieved_goal\"].shape)\n",
    "\n",
    "# B, the number of subtrajectories per batch (from paper)\n",
    "B = 100\n",
    "\n",
    "# T, the length of each subtrajectory (from paper)\n",
    "T = 40\n",
    "\n",
    "# AntMaze state and action dims (from Minari)\n",
    "state_dim = 31\n",
    "action_dim = 8\n",
    "\n",
    "# Initialize the models\n",
    "q_phi = SkillPosterior(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "pi_theta = SkillPolicy(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "p_psi = TAWM(state_dim=state_dim).to(device)\n",
    "p_omega = SkillPrior(state_dim=state_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89929230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_episode_splits(minari_dataset, train=0.8, val=0.1, test=0.1, seed=0):\n",
    "    \"\"\"Return three lists of episode indices (train_ids, val_ids, test_ids).\"\"\"\n",
    "    # Materialize all episodes once so we know how many there are\n",
    "    episodes = list(minari_dataset.iterate_episodes())\n",
    "    n = len(episodes)\n",
    "    idxs = list(range(n))\n",
    "    # Shuffle the indices\n",
    "    random.Random(seed).shuffle(idxs)\n",
    "    n_train = int(round(train * n))\n",
    "    n_val = int(round(val * n))\n",
    "    train_ids = idxs[:n_train]\n",
    "    val_ids = idxs[n_train:n_train+n_val]\n",
    "    test_ids = idxs[n_train+n_val:]\n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "class SubtrajDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loops over minari_dataset.iterate_episodes(), but keeps only episodes whose index is in episode_ids\n",
    "    \"\"\"\n",
    "    def __init__(self, minari_dataset, T, episode_ids, stride=3):\n",
    "        self.T = T\n",
    "        self.items = []  \n",
    "\n",
    "        # Iterate all episodes but only process those whose global index is in episode_ids\n",
    "        for ep_idx, ep in enumerate(minari_dataset.iterate_episodes()):\n",
    "            if ep_idx not in set(episode_ids):\n",
    "                continue\n",
    "            obs = ep.observations[\"observation\"]          \n",
    "            ach = ep.observations[\"achieved_goal\"]        \n",
    "            act = ep.actions                               \n",
    "            Ltot = len(obs)\n",
    "            if Ltot < T + 1:\n",
    "                continue\n",
    "\n",
    "            state_ext = np.concatenate([obs, ach], axis=-1).astype(np.float32)\n",
    "            for t in range(0, Ltot - T, stride):\n",
    "                state_seq = state_ext[t:t+T]         \n",
    "                s0 = state_seq[0]             \n",
    "                action_seq = act[t:t+T].astype(np.float32)  \n",
    "                sT = state_ext[t+T]           \n",
    "                self.items.append((s0, state_seq, action_seq, sT))\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"standardize s0, state_sequence, and sT by (x - mean) / std\"\"\"\n",
    "        \n",
    "        s0, S, A, sT = self.items[i]\n",
    "        if hasattr(self, \"stats\") and self.stats is not None:\n",
    "            S_mean, S_std = self.stats\n",
    "            S  = (S  - S_mean) / S_std\n",
    "            s0 = (s0 - S_mean) / S_std\n",
    "            sT = (sT - S_mean) / S_std\n",
    "            A  = A\n",
    "        return {\n",
    "            \"s0\": torch.as_tensor(s0, dtype=torch.float32),\n",
    "            \"state_sequence\": torch.as_tensor(S, dtype=torch.float32),\n",
    "            \"action_sequence\": torch.as_tensor(A, dtype=torch.float32),\n",
    "            \"sT\": torch.as_tensor(sT, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "def collate(batch):\n",
    "    return {\n",
    "        \"s0\": torch.stack([b[\"s0\"] for b in batch], 0),\n",
    "        \"state_sequence\": torch.stack([b[\"state_sequence\"] for b in batch], 0),\n",
    "        \"action_sequence\": torch.stack([b[\"action_sequence\"] for b in batch], 0),\n",
    "        \"sT\": torch.stack([b[\"sT\"] for b in batch], 0),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6639cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:800  val:0  test:200\n",
      "train:768800  val:0  test:192200\n"
     ]
    }
   ],
   "source": [
    "# Pick indices for train/test/split\n",
    "train_ids, val_ids, test_ids = make_episode_splits(ant_maze_dataset, train=0.8, val=0.0, test=0.2, seed=0)\n",
    "print(f\"train:{len(train_ids)}  val:{len(val_ids)}  test:{len(test_ids)}\")\n",
    "\n",
    "# Datasets from episode subsets\n",
    "train_ds = SubtrajDataset(ant_maze_dataset, T=T, episode_ids=train_ids, stride=1)\n",
    "val_ds = SubtrajDataset(ant_maze_dataset, T=T, episode_ids=val_ids,   stride=1)\n",
    "test_ds = SubtrajDataset(ant_maze_dataset, T=T, episode_ids=test_ids,  stride=1)  \n",
    "\n",
    "print(f\"train:{len(train_ds)}  val:{len(val_ds)}  test:{len(test_ds)}\")\n",
    "\n",
    "# find per-feature mean and std from all state_sequence timesteps in train_ds\n",
    "def compute_stats(ds):\n",
    "    Ss = []\n",
    "    for item in ds.items:\n",
    "        Ss.append(item[1])  # state_sequence [T,29]\n",
    "    S = np.concatenate([x.reshape(-1, x.shape[-1]) for x in Ss], axis=0)\n",
    "    S_mean, S_std = S.mean(0), S.std(0) + 1e-6\n",
    "    return (S_mean, S_std)\n",
    "\n",
    "S_mean, S_std = 0, 1\n",
    "\n",
    "# pass stats into datasets\n",
    "train_ds.stats = (S_mean, S_std)\n",
    "val_ds.stats = (S_mean, S_std)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=B, shuffle=True,  collate_fn=collate, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=B, shuffle=False, collate_fn=collate, drop_last=False)\n",
    "\n",
    "test_ds.stats = (S_mean, S_std)\n",
    "test_loader = DataLoader(test_ds, batch_size=B, shuffle=False, collate_fn=collate, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = 1.0, 1.0  \n",
    "\n",
    "def compute_loss(batch):\n",
    "    s0, S, A, sT = batch[\"s0\"], batch[\"state_sequence\"], batch[\"action_sequence\"], batch[\"sT\"]\n",
    "    B, T, _  = S.shape\n",
    "    denom = B * T\n",
    "\n",
    "    # State encoder\n",
    "    mu_q, std_q = q_phi(S, A)                      \n",
    "    z = mu_q + std_q * torch.randn_like(mu_q)\n",
    "\n",
    "    # Low-level policy pi_theta(a|s,z)\n",
    "    z_bt = z.unsqueeze(1).expand(B, T, -1)         \n",
    "    mu_pi, std_pi = pi_theta(S.reshape(B*T, -1), z_bt.reshape(B*T, -1))\n",
    "    mu_pi, std_pi = mu_pi.view(B, T, -1), std_pi.view(B, T, -1)\n",
    "    a_dist  = Independent(Normal(mu_pi, std_pi), 1)        \n",
    "\n",
    "    # Compute policy loss\n",
    "    a_loss  = -a_dist.log_prob(A).sum() / denom\n",
    "    \n",
    "    mu_pr, std_pr = p_omega(s0)                              \n",
    "    prior_dist = Independent(Normal(mu_pr, std_pr), 1)\n",
    "    log_prior = prior_dist.log_prob(z).sum() / denom\n",
    "    post_dist = Independent(Normal(mu_q,  std_q),  1)\n",
    "    log_post  = post_dist.log_prob(z).sum() / denom\n",
    "    \n",
    "    # Compute KL loss\n",
    "    kl_loss = - log_prior + log_post\n",
    "\n",
    "    # Detach gradient\n",
    "    z_detached = z.detach()\n",
    "\n",
    "    # TAWM over terminal state\n",
    "    mu_T, std_T = p_psi(s0, z_detached)                            \n",
    "    sT_dist = Independent(Normal(mu_T, std_T), 1)\n",
    "\n",
    "    # State decoder loss\n",
    "    sT_loss = -sT_dist.log_prob(sT).sum() / denom\n",
    "\n",
    "    # Overall loss (naive VI loss from paper)\n",
    "    loss = alpha * sT_loss + a_loss + beta * kl_loss\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"policy_loss\": a_loss,\n",
    "        \"kl_loss\": kl_loss,\n",
    "        \"state_decoder_loss\": sT_loss\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(val_loader, q_phi, pi_theta, p_psi, p_omega, device):\n",
    "    \"\"\"Compute validation loss\"\"\"\n",
    "    q_phi.eval()\n",
    "    pi_theta.eval()\n",
    "    p_psi.eval()\n",
    "    p_omega.eval()\n",
    "    loss_sum,policy_loss_sum, kl_loss_sum, state_decoder_loss_sum, n = 0.0, 0.0, 0.0, 0.0, 0\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        terms = compute_loss(batch)\n",
    "        loss = terms[\"loss\"]\n",
    "        policy_loss = terms[\"policy_loss\"]\n",
    "        kl_loss = terms[\"kl_loss\"]\n",
    "        state_decoder_loss = terms[\"state_decoder_loss\"]\n",
    "        loss_sum += float(loss.item())\n",
    "        policy_loss_sum += float(policy_loss.item())\n",
    "        kl_loss_sum += float(kl_loss.item())\n",
    "        state_decoder_loss_sum += float(state_decoder_loss.item())\n",
    "\n",
    "        n += 1\n",
    "    if n == 0: \n",
    "        return None, None, None, None\n",
    "    return loss_sum / n, policy_loss_sum / n, kl_loss_sum / n, state_decoder_loss_sum / n\n",
    "\n",
    "def skill_model_training_with_val(\n",
    "    train_loader, val_loader,\n",
    "    q_phi, pi_theta, p_psi, p_omega,\n",
    "    lr=5e-5,\n",
    "    epochs=50, steps=1, grad_clip=1.0\n",
    "):\n",
    "    q_phi.to(device)\n",
    "    pi_theta.to(device)\n",
    "    p_psi.to(device)\n",
    "    p_omega.to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(list(q_phi.parameters()) + list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), lr=lr)\n",
    "\n",
    "    tr, va = [], []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        q_phi.train()\n",
    "        pi_theta.train()\n",
    "        p_psi.train()\n",
    "        p_omega.train()\n",
    "        loss_run, policy_loss_run, kl_loss_run, state_decoder_loss_run = 0.0, 0.0, 0.0, 0.0 # Running loss in current epoch\n",
    "\n",
    "        nb = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Rebuilds dictionary but moves tensors to the device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            nb += 1\n",
    "\n",
    "            for _ in range(steps):\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                terms = compute_loss(batch)\n",
    "                loss = terms[\"loss\"]\n",
    "                policy_loss = terms[\"policy_loss\"]\n",
    "                kl_loss = terms[\"kl_loss\"]\n",
    "                state_decoder_loss = terms[\"state_decoder_loss\"]\n",
    "                loss.backward()\n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(list(q_phi.parameters()) + list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), grad_clip)\n",
    "                opt.step()\n",
    "            loss_run += float(loss.item())\n",
    "            policy_loss_run += float(policy_loss.item())\n",
    "            kl_loss_run += float(kl_loss.item())\n",
    "            state_decoder_loss_run += float(state_decoder_loss.item())\n",
    "\n",
    "\n",
    "        # Calculate the average losses over all the batches in the epoch\n",
    "        loss_epoch = loss_run / max(1, nb)\n",
    "        policy_loss_epoch = policy_loss_run / max(1, nb)\n",
    "        kl_loss_epoch = kl_loss_run / max(1, nb)\n",
    "        state_decoder_loss_epoch = state_decoder_loss_run / max(1, nb)\n",
    "\n",
    "        tr.append(loss_epoch)\n",
    "\n",
    "        # validation\n",
    "        v_loss, v_policy_loss, v_kl_loss, v_state_decoder_loss = eval_epoch(val_loader, q_phi, pi_theta, p_psi, p_omega, device)\n",
    "        va.append(v_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch:03d}/{epochs}] \"\n",
    "              f\"train loss:{loss_epoch:.4f} \"\n",
    "              f\"| val loss:{v_loss:.4f}\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"train/loss\": loss_epoch,\n",
    "            \"train/policy_loss\": policy_loss_epoch,\n",
    "            \"train/kl_loss\": kl_loss_epoch,\n",
    "            \"train/state_decoder_loss\": state_decoder_loss_epoch,\n",
    "            \"val/loss\": v_loss,\n",
    "            \"val/policy_loss\": v_policy_loss,\n",
    "            \"val/kl_loss\": v_kl_loss,\n",
    "            \"val/state_decoder_loss\": v_state_decoder_loss,\n",
    "            \"epoch\": epoch\n",
    "        }, step=epoch)\n",
    "\n",
    "    plt.figure(figsize=(7.5,4.5))\n",
    "    plt.plot(tr, label=\"Train loss\")\n",
    "    if all(v is not None for v in va):\n",
    "        plt.plot(va, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(\"EM training: train vs. val losses\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    wandb.log({\"plots/loss_curves\": wandb.Image(fig)}, step=epoch)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return {\"train_loss\": tr, \"val_E\": va}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9163f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwilliam-huang-08\u001b[0m (\u001b[33mwilliam-huang-08-yale-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/williamhuang/Documents/Yale/Research/CMUBiorobotics/OPOSM/wandb/run-20251124_170647-3xcm1bq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning/runs/3xcm1bq3' target=\"_blank\">antmaze-medium_em</a></strong> to <a href='https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning' target=\"_blank\">https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning/runs/3xcm1bq3' target=\"_blank\">https://wandb.ai/william-huang-08-yale-university/tawm-skill-learning/runs/3xcm1bq3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"tawm-skill-learning\",\n",
    "    name=\"antmaze-medium_em\",\n",
    "    config=dict(\n",
    "        B=B, T=T, Z_DIM=Z_DIM, NUM_NEURONS=NUM_NEURONS,\n",
    "        e_lr=5e-5, m_lr=5e-5, e_steps=1, m_steps=1,\n",
    "        dataset=\"D4RL/antmaze/medium-diverse-v1\",\n",
    "        device=device\n",
    "    )\n",
    ")\n",
    "\n",
    "wandb.watch([q_phi, pi_theta, p_psi, p_omega], log=\"gradients\", log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "curves = skill_model_training_with_val(train_loader, test_loader, q_phi, pi_theta, p_psi, p_omega, epochs=100, e_lr=5e-5, m_lr=5e-5, e_steps=1, m_steps=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oposm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
