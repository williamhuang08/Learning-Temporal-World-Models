{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edb0f36",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Model-Based Temporal Abstraction involves simultaneuously learning\n",
    "1) skill-conditioned low-level policy\n",
    "2) skill-conditioned temporally abstract world model\n",
    "\n",
    "Notation\n",
    "- skill-conditioned low-level policy: $\\pi_{\\theta}(a_t|s_t, z)$\n",
    "    - $\\theta$ are parameters\n",
    "    - $a_t \\in A$ is current action selected by agent\n",
    "    - $s_t \\in S$ is current state\n",
    "    - $z \\in Z$ is abstract skill variable that encodes a skill\n",
    "\n",
    "- skill-conditioned temporally abstract world model (TAWM): $p_{\\psi}(s'|s,z)$ (models distribution of states agent is in after skill $z$)\n",
    "    - $\\psi$ parameters\n",
    "    - $z$ is current skill\n",
    "\n",
    "Note: low-level policy and TAWM not trained on rewards, reward function is provided later for planning with the learned skills \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages for the whole script\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Independent, kl_divergence\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import minari\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4bb13",
   "metadata": {},
   "source": [
    "### Learning $\\pi_{\\theta}$ and $p_{\\psi}$\n",
    "\n",
    "Learning $\\pi_{\\theta}$ and $p_{\\psi}$ requires treating skills as latent variables and optimizing the ELBO\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\psi,\\phi,\\omega)\n",
    "= \\mathbb{E}_{\\tau_T \\sim \\mathcal{D}}\\!\\left[\n",
    "  \\mathbb{E}_{q_\\phi(z\\,|\\,\\tau_T)}\\!\\left[\n",
    "    \\log \\pi_\\theta(\\bar{a}\\,|\\,\\bar{s}, z)\n",
    "    + \\log p_\\psi(s_T \\,|\\, s_0, z)\n",
    "  \\right]\n",
    "  - D_{\\mathrm{KL}}\\!\\left(q_\\phi(z\\,|\\,\\tau_T)\\,\\|\\,p_\\omega(z\\,|\\,s_0)\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "where $\\tau_T$ is a T-length subtrajectory sampled from the offline dataset $\\mathcal{D}$, $\\bar{s}$ and $\\bar{a}$ are state and action sequences of $\\tau_T$, $q_{\\psi}$ is a posterior over $z$ given $\\tau_T$, and $p_{\\omega}$ is a prior of $z$ given $s_0$.\n",
    "\n",
    "The first term is the log-likelihood of demonstrator actions. This ensures that the low-level policy can reproduce a demonstrator's action sequence given a skill. This forces $z$ to encode control-relevant information.\n",
    "\n",
    "The second term is the log-likelihood of long-term state transitions. This term ensures that we learn relationships between $z$ to what possible $s_T$ could result from. the skill.\n",
    "\n",
    "Finally, the last term is the KL divergence between skill posterior and prior (encourages compression of skills). Therefore, maximizing this ELBO makes skills $z$ explain the data and keeps the KL divergence small. This ensures that the skill is start-state predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "695e8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the paper, each layer contains 256 neurons\n",
    "NUM_NEURONS = 256\n",
    "# The dimension of the abstract skill variable, z\n",
    "Z_DIM = 256\n",
    "\n",
    "# Skill Posterior, q_phi\n",
    "class SkillPosterior(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: sequence of skills and actions\n",
    "    Output: mean and std over z\n",
    "\n",
    "    1. Linear layer w/ ReLU activation for the state sequence\n",
    "    2. Single-layer bidirectional GRU for embedded states and action sequence (concatenated)\n",
    "    3. Extract mean and std of layer 2's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bi_gru = nn.GRU(input_size=NUM_NEURONS+self.action_dim, hidden_size= NUM_NEURONS//2, bidirectional=True, batch_first=True)\n",
    "        self.mean = MeanNetwork(Z_DIM)\n",
    "        self.std = StandardDeviationNetwork(Z_DIM)\n",
    "\n",
    "    def forward(self, state_sequence, action_sequence):\n",
    "        embedded_states = self.relu(self.fc1(state_sequence))\n",
    "        concatenated = torch.cat([embedded_states, action_sequence], dim=-1)\n",
    "        x, _ = self.bi_gru(concatenated) # [B, T, NUM_NEURONS]\n",
    "        seq_emb = x.mean(dim=1) # [B, NUM_NEURONS]\n",
    "        mean = self.mean.forward(seq_emb)\n",
    "        std = self.std.forward(seq_emb)\n",
    "        return mean, std\n",
    "\n",
    "# Low-Level Skill-Conditioned Policy, pi_theta\n",
    "class SkillPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: Current state and a skill, z\n",
    "    Output: mean and std over a\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for the state and abstract skill (concatenated)\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(self.action_dim)\n",
    "        self.std = StandardDeviationNetwork(self.action_dim)\n",
    "    \n",
    "    def forward(self, state, z):\n",
    "        c = torch.cat([state, z], dim=-1)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "        \n",
    "\n",
    "# Temporally-Abstract World Model, p_psi\n",
    "class TAWM(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: initial state, along with the abstract skill\n",
    "    Output: mean and std over terminal state\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for initial state and abstract skill (concatenated)\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(self.state_dim)\n",
    "        self.std = StandardDeviationNetwork(self.state_dim)\n",
    "    \n",
    "    def forward(self, input_state, z):\n",
    "        c = torch.cat([input_state, z], dim=-1)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "\n",
    "# Skill Prior, p_omega\n",
    "class SkillPrior(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: Initial state, s0, in the trajectory\n",
    "    Output: mean and std over the abstract skill, z\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for the initial state\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(Z_DIM)\n",
    "        self.std = StandardDeviationNetwork(Z_DIM)\n",
    "    \n",
    "    def forward(self, input_state):\n",
    "        x = self.relu(self.fc1(input_state))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "\n",
    "class MeanNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: tensor to calculate mean\n",
    "    Output: mean of input w/ dimension out_dim\n",
    "\n",
    "    1. 2-layer network w/ ReLU activation for the first layer\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class StandardDeviationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: tensor to calculate std\n",
    "    Output: std of input w/ dimension out_dim\n",
    "\n",
    "    Note: the standard deviation is lower and upper bounded at 0.05 and 2.0\n",
    "    - if std is 0, then log(std) -> inf\n",
    "    - if std is large, then can affect training\n",
    "\n",
    "    1. 2-layer linear network with ReLU activation after first layer and softplus after second\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim, min_std=0.05, max_std=2.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.min_std = min_std\n",
    "        self.max_std = max_std\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        std = self.softplus(x) + self.min_std  # lower bound\n",
    "        std = torch.clamp(std, max=self.max_std)\n",
    "        return std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4f8a0",
   "metadata": {},
   "source": [
    "#### The Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "Since calculating the true posterior of $z$ given $\\tau_T$ is intractable, we infer $q_{\\psi}(z|\\tau_T)$.\n",
    "\n",
    "1. E-Step:\n",
    "- Update $\\psi$ w/gradient descent so that KL divergence between $q_\\psi$ and true posterior is minimized\n",
    "\n",
    "2. M-Step:\n",
    "- Fixing $q_{\\psi}$, update ($\\theta, \\psi, \\omega$) s.t. ELBO is maximized using gradient ascent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c61868",
   "metadata": {},
   "source": [
    "##### E-Step (Update $\\psi$)\n",
    "\n",
    "In this step, we want to minimize \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{T}_T\\sim\\mathcal{D}}\n",
    "\\Bigg[\n",
    "\\mathbb{E}_{z\\sim q_\\phi}\n",
    "\\bigg[\n",
    "\\log \\frac{q_\\phi\\!\\left(z\\mid \\bar{s},\\bar{a}\\right)}\n",
    "{\\pi_\\theta\\!\\left(\\bar{a}\\mid \\bar{s}, z\\right)\\,p_\\omega\\!\\left(z\\mid s_0\\right)}\n",
    "\\bigg]\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "Equivalently, we want to minimize $\\mathcal{KL}(q_{\\psi}||p)$ where $p$, the true posterior is \n",
    "\n",
    "$$\n",
    "p(z \\mid \\bar{s}, \\bar{a}) = \\frac{1}{\\eta}\\,\\pi_\\theta(\\bar{a}\\mid \\bar{s}, z)\\,p_\\omega(z\\mid s_0).\n",
    "$$\n",
    "\n",
    "\n",
    "##### M-Step (Update $\\theta$, $\\psi$, $\\omega$)\n",
    "\n",
    "In this step, we want to update $\\theta$, $\\psi$, and $\\omega$ using gradient ascent to maximize the ELBO from above.\n",
    "\n",
    "Both steps are trained using an Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae831ff",
   "metadata": {},
   "source": [
    "##### Dataset 1: AntMaze Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "c4a079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the AntMaze dataset in Minari format\n",
    "ant_maze_dataset = minari.load_dataset('D4RL/antmaze/medium-play-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "1cd992f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "dict_keys(['achieved_goal', 'desired_goal', 'observation'])\n",
      "(1001, 27)\n",
      "(1001, 2)\n"
     ]
    }
   ],
   "source": [
    "print(ant_maze_dataset[0].actions.shape)\n",
    "print(ant_maze_dataset[0].observations.keys())\n",
    "print(ant_maze_dataset[0].observations[\"observation\"].shape)\n",
    "print(ant_maze_dataset[0].observations[\"achieved_goal\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "93eff05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, the number of subtrajectories per batch (from paper)\n",
    "B = 100\n",
    "\n",
    "# T, the length of each subtrajectory (from paper)\n",
    "T = 9\n",
    "\n",
    "# AntMaze state and action dims (from Minari)\n",
    "state_dim = 29\n",
    "action_dim = 8\n",
    "\n",
    "# Initialize the models\n",
    "q_phi = SkillPosterior(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "pi_theta = SkillPolicy(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "p_psi = TAWM(state_dim=state_dim).to(device)\n",
    "p_omega = SkillPrior(state_dim=state_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "d3c71a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Input: LoadedMinari dataset and the length of the number of actions in each subtrajectory (T)\n",
    "    Output: Dictionary with keys \"s0, state_sequence, action_sequence, and sT\"\n",
    "\n",
    "    Finds all episodes that have at least T actions. Then, for each of those episodes, creates a sliding window to create subtrajectories w/ T actions\n",
    "    \"\"\"\n",
    "    def __init__(self, ant_maze_dataset, T):\n",
    "        self.T = T\n",
    "        self.subtrajectories = []\n",
    "\n",
    "        for ep in ant_maze_dataset.iterate_episodes():\n",
    "            s = ep.observations[\"observation\"]\n",
    "            a = ep.actions\n",
    "            l = len(s)\n",
    "            if l < T + 1:\n",
    "                continue\n",
    "            \n",
    "            # Consider skipping timesteps so that subtrajectories don't overlap that much\n",
    "            stride = 3\n",
    "            for t in range(0, l - T, stride):\n",
    "                obs = ep.observations[\"observation\"][t:t+T+1] # (27,)\n",
    "                ach = ep.observations[\"achieved_goal\"][t:t+T+1]  # (2,)\n",
    "                state_sequence_extended = np.concatenate([obs, ach], axis=-1)\n",
    "                state_sequence = state_sequence_extended[:-1]\n",
    "                s0 = state_sequence[0]\n",
    "                action_sequence = a[t: t + T]\n",
    "                sT = state_sequence_extended[-1]\n",
    "                self.subtrajectories.append((s0, state_sequence, action_sequence, sT))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subtrajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s0, state_sequence, action_sequence, sT = self.subtrajectories[idx]\n",
    "\n",
    "        return {\n",
    "            \"s0\": torch.as_tensor(s0, dtype=torch.float32),\n",
    "            \"state_sequence\": torch.as_tensor(state_sequence, dtype=torch.float32),\n",
    "            \"action_sequence\": torch.as_tensor(action_sequence, dtype=torch.float32),\n",
    "            \"sT\": torch.as_tensor(sT, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "def collate(batch):\n",
    "    # Vertically stacks each of the components of the subtrajectories such that the first dimension is the batch\n",
    "    return {\n",
    "        \"s0\": torch.stack([b[\"s0\"] for b in batch], 0),\n",
    "        \"state_sequence\": torch.stack([b[\"state_sequence\"] for b in batch], 0),\n",
    "        \"action_sequence\": torch.stack([b[\"action_sequence\"] for b in batch], 0),\n",
    "        \"sT\": torch.stack([b[\"sT\"] for b in batch], 0)\n",
    "    }\n",
    "\n",
    "# Create the dictionary of subtrajectories\n",
    "dataset = TrainingDataset(ant_maze_dataset, T)\n",
    "\n",
    "# Iterator w/ groups of subtrajectories from the dataset of size B \n",
    "loader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "5fcad857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_e_loss(batch):\n",
    "    s0  = batch[\"s0\"]               \n",
    "    S   = batch[\"state_sequence\"]   \n",
    "    A   = batch[\"action_sequence\"]  \n",
    "    Bsz, T, _ = S.shape\n",
    "\n",
    "    # Sampling z using the reparameterization trick where z = mu(tau) + std(tau) * epsilon\n",
    "    mu_q, std_q = q_phi(S, A) # [B, z_dim]\n",
    "    eps = torch.randn_like(mu_q)\n",
    "    z = (mu_q + std_q * eps)      \n",
    "\n",
    "    # Freeze the weights of the low-level skill-conditioned policy, pi_theta\n",
    "    with torch.no_grad():\n",
    "        z_bt = z.unsqueeze(1).expand(Bsz, T, -1) # [B, T, z_dim]\n",
    "        # Flatten time dimension since pi_theta does not the expect extra dim ([B * T, s_dim])\n",
    "        Sf = S.reshape(Bsz*T, -1)\n",
    "        Zf = z_bt.reshape(Bsz*T, -1)\n",
    "        mu_pi, std_pi = pi_theta(Sf, Zf)     \n",
    "        # Restore time dimension ([B, T, a_dim])\n",
    "        mu_pi  = mu_pi.view(Bsz, T, -1)\n",
    "        std_pi = std_pi.view(Bsz, T, -1)\n",
    "        # Build a MVN over independent actions at each timestep of each batch and sum log_probs across action_dim\n",
    "        pi_dist = Independent(Normal(mu_pi, std_pi), 1)\n",
    "        # Compute the log probability of observed actions\n",
    "        log_pi = pi_dist.log_prob(A) # ([B, T])    \n",
    "\n",
    "    # Freeze the weights of the skill prior, p_omega\n",
    "    with torch.no_grad():\n",
    "        # Find the distribution of the abstract skill given start states (mu_pr & std_pr: [B, z_dim])\n",
    "        mu_pr, std_pr = p_omega(s0)  \n",
    "        # Build a MVN over independent skills at each timestep of each batch and sum log_probs across z_dim\n",
    "        prior_dist = Independent(Normal(mu_pr, std_pr), 1)\n",
    "        # Compute the log-probability over the sampled skills using the prior\n",
    "        log_p_omega = prior_dist.log_prob(z)   \n",
    "\n",
    "    post_dist = Independent(Normal(mu_q, std_q), 1)\n",
    "    # Compute the log-probability over the sampled skills using the inferred posterior\n",
    "    log_q = post_dist.log_prob(z)              \n",
    "\n",
    "    # Calculate the E-objective\n",
    "    e_obj = (log_pi.sum(dim=1) + log_p_omega - log_q)\n",
    "    e_loss = -e_obj.mean() # minimize the negative objective\n",
    "    return e_loss\n",
    "\n",
    "\n",
    "\n",
    "def compute_m_loss(batch):\n",
    "    s0  = batch[\"s0\"]               \n",
    "    S   = batch[\"state_sequence\"]   \n",
    "    A   = batch[\"action_sequence\"] \n",
    "    sT  = batch[\"sT\"]               \n",
    "    Bsz, T, Sdim = S.shape\n",
    "\n",
    "    # Freeze the weights of the inferred posterior, only update omega, psi, and theta\n",
    "    with torch.no_grad():\n",
    "        mu_q, std_q = q_phi(S, A)        # [B, z_dim]\n",
    "        eps = torch.randn_like(mu_q)\n",
    "        # same reparameterization trick as E-loss\n",
    "        z = (mu_q + std_q * eps)  \n",
    "\n",
    "    z_bt = z.unsqueeze(1).expand(Bsz, T, -1)\n",
    "    # Flatten time dimension since pi_theta does not the expect extra dim ([B * T, s_dim])\n",
    "    Sf = S.reshape(Bsz*T, -1)\n",
    "    Zf = z_bt.reshape(Bsz*T, -1)\n",
    "    mu_pi, std_pi = pi_theta(Sf, Zf)    \n",
    "    # Restore time dimension ([B, T, a_dim])     \n",
    "    mu_pi = mu_pi.reshape(Bsz, T, -1)\n",
    "    std_pi = std_pi.reshape(Bsz, T, -1)\n",
    "    # Build a MVN over independent actions at each timestep of each batch and sum log_probs across action_dim\n",
    "    pi_dist = Independent(Normal(mu_pi, std_pi), 1)\n",
    "    log_pi = pi_dist.log_prob(A)               \n",
    "\n",
    "    # Zs: [B, T, z_dim]\n",
    "    mu_T, std_T = p_psi(s0, z)\n",
    "    # Build a MVN over independent terminal states at each timestep of each batch and sum log_probs across state_dim     \n",
    "    ppsi_dist = Independent(Normal(mu_T, std_T), 1)\n",
    "    # Compute the log-probability over the observed terminal states using the TAWM\n",
    "    log_p_psi = ppsi_dist.log_prob(sT)          \n",
    "\n",
    "    # Find the distribution of the abstract skill given start states (mu_pr & std_pr: [B, z_dim])\n",
    "    mu_pr, std_pr = p_omega(s0)                \n",
    "    # Build a MVN over independent skills at each timestep of each batch and sum log_probs across z_dim\n",
    "    prior_dist = Independent(Normal(mu_pr, std_pr), 1)\n",
    "    # Compute the log-probability over the sampled skills using the prior\n",
    "    log_p_omega = prior_dist.log_prob(z)   \n",
    "\n",
    "    # sum to find join log-likelihood of the whole sequence\n",
    "    obj = log_pi.sum(dim=1) + log_p_psi + log_p_omega   \n",
    "    m_loss = -obj.mean()\n",
    "    return m_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def skill_model_training(\n",
    "    loader,\n",
    "    q_phi, pi_theta, p_psi, p_omega,\n",
    "    e_lr=5e-5, m_lr=5e-5,\n",
    "    epochs=50,\n",
    "    e_steps=1, m_steps=1,\n",
    "    grad_clip=1.0 # prevents runaway gradients\n",
    "):\n",
    "    # Skill model training setup\n",
    "    q_phi.to(device)\n",
    "    pi_theta.to(device)\n",
    "    p_psi.to(device)\n",
    "    p_omega.to(device)\n",
    "\n",
    "    e_optimizer = torch.optim.Adam(q_phi.parameters(), lr=e_lr)\n",
    "    m_optimizer = torch.optim.Adam(list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), lr=m_lr)\n",
    "\n",
    "    e_curve = []   \n",
    "    m_curve = []   \n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Running e_loss, m_loss, and batches in current epoch\n",
    "        e_running, m_running, nb = 0.0, 0.0, 0\n",
    "\n",
    "        for batch in loader:\n",
    "            # Rebuilds dictionary but moves tensors to the device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            nb += 1\n",
    "\n",
    "            # In E-step, train the posterior while freezing other parameters\n",
    "            q_phi.train()\n",
    "            pi_theta.eval()\n",
    "            p_psi.eval()\n",
    "            p_omega.eval()\n",
    "\n",
    "            for p in q_phi.parameters(): \n",
    "                p.requires_grad_(True)\n",
    "            for m in (pi_theta, p_psi, p_omega):\n",
    "                for p in m.parameters(): \n",
    "                    p.requires_grad_(False)\n",
    "\n",
    "            # For the e-step, \n",
    "            for _ in range(e_steps):\n",
    "                # Resent gradients\n",
    "                e_optimizer.zero_grad(set_to_none=True)\n",
    "                e_loss = compute_e_loss(batch)\n",
    "                e_loss.backward() \n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(q_phi.parameters(), grad_clip)\n",
    "                e_optimizer.step() # Update the parameters of the posterior\n",
    "\n",
    "            e_running += e_loss.item()\n",
    "\n",
    "            # Freeze posterior weights, update all other weights\n",
    "            q_phi.eval()\n",
    "            pi_theta.train()\n",
    "            p_psi.train()\n",
    "            p_omega.train()\n",
    "\n",
    "            for p in q_phi.parameters(): \n",
    "                p.requires_grad_(False)\n",
    "            for m in (pi_theta, p_psi, p_omega):\n",
    "                for p in m.parameters(): \n",
    "                    p.requires_grad_(True)\n",
    "\n",
    "            for _ in range(m_steps):\n",
    "                # Reset gradients\n",
    "                m_optimizer.zero_grad(set_to_none=True)\n",
    "                m_loss = compute_m_loss(batch)\n",
    "                m_loss.backward()\n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), grad_clip)\n",
    "                m_optimizer.step() # Update theta, psi, and omega\n",
    "\n",
    "            m_running += m_loss.item()\n",
    "\n",
    "        # Calculate the average losses over all the batches in the epoch\n",
    "        e_epoch = e_running / max(1, nb)\n",
    "        m_epoch = m_running / max(1, nb)\n",
    "        e_curve.append(e_epoch)\n",
    "        m_curve.append(m_epoch)\n",
    "        print(f\"[Epoch {epoch:03d}/{epochs}]  E: {e_epoch:.4f}   M: {m_epoch:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(e_curve, label=\"E-loss\")\n",
    "    plt.plot(m_curve, label=\"M-loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"EM training losses\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return e_curve, m_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "7f6f2da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001/25]  E: 47.1526   M: 612.1870\n",
      "[Epoch 002/25]  E: 31.5192   M: 588.5160\n",
      "[Epoch 003/25]  E: 24.8898   M: 576.8893\n",
      "[Epoch 004/25]  E: 21.0094   M: 569.8205\n",
      "[Epoch 005/25]  E: 18.4950   M: 564.6932\n",
      "[Epoch 006/25]  E: 16.6712   M: 560.8886\n",
      "[Epoch 007/25]  E: 15.2432   M: 558.1020\n",
      "[Epoch 008/25]  E: 14.0861   M: 555.8779\n",
      "[Epoch 009/25]  E: 13.1440   M: 554.0309\n",
      "[Epoch 010/25]  E: 12.3362   M: 552.3174\n",
      "[Epoch 011/25]  E: 11.6588   M: 551.0306\n",
      "[Epoch 012/25]  E: 11.0686   M: 549.7999\n",
      "[Epoch 013/25]  E: 10.5685   M: 548.7119\n",
      "[Epoch 014/25]  E: 10.1259   M: 547.9745\n",
      "[Epoch 015/25]  E: 9.7445   M: 546.9560\n",
      "[Epoch 016/25]  E: 9.4035   M: 546.2930\n",
      "[Epoch 017/25]  E: 9.0964   M: 545.6731\n",
      "[Epoch 018/25]  E: 8.8150   M: 545.1026\n",
      "[Epoch 019/25]  E: 8.5697   M: 544.4341\n",
      "[Epoch 020/25]  E: 8.3305   M: 543.9675\n",
      "[Epoch 021/25]  E: 8.0972   M: 543.5393\n",
      "[Epoch 022/25]  E: 7.9058   M: 543.1313\n",
      "[Epoch 023/25]  E: 7.7034   M: 542.7275\n",
      "[Epoch 024/25]  E: 7.5208   M: 542.4650\n",
      "[Epoch 025/25]  E: 7.3560   M: 542.1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wp/q7yyy4ls2m3bvyscgh359t0r0000gn/T/ipykernel_37415/2482164681.py:93: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.tight_layout(); plt.show()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([47.15259306121089,\n",
       "  31.51918206171687,\n",
       "  24.889834441447185,\n",
       "  21.00940923575548,\n",
       "  18.495002500240176,\n",
       "  16.67117750522233,\n",
       "  15.243227473486586,\n",
       "  14.086138733924336,\n",
       "  13.14402588674309,\n",
       "  12.336182354584201,\n",
       "  11.658840118001955,\n",
       "  11.068605773110404,\n",
       "  10.568502830522657,\n",
       "  10.125925442225263,\n",
       "  9.74453379175094,\n",
       "  9.403523720337185,\n",
       "  9.096398457538145,\n",
       "  8.814963859769874,\n",
       "  8.569735810826913,\n",
       "  8.3305361769632,\n",
       "  8.097151646864377,\n",
       "  7.905775060881143,\n",
       "  7.703362999401784,\n",
       "  7.520768780740727,\n",
       "  7.356005560852069],\n",
       " [612.1869514580579,\n",
       "  588.5159956917662,\n",
       "  576.8893495127513,\n",
       "  569.8204593531675,\n",
       "  564.6932217416446,\n",
       "  560.8886068753245,\n",
       "  558.1020340933901,\n",
       "  555.8779325087625,\n",
       "  554.0308801011377,\n",
       "  552.3174048847302,\n",
       "  551.030571901258,\n",
       "  549.7999219819499,\n",
       "  548.7119467375142,\n",
       "  547.9745004152603,\n",
       "  546.9560301443602,\n",
       "  546.2930223908669,\n",
       "  545.6730867414676,\n",
       "  545.1025967600842,\n",
       "  544.4340908269508,\n",
       "  543.9674537128552,\n",
       "  543.5393139058369,\n",
       "  543.1312514198511,\n",
       "  542.7275133761034,\n",
       "  542.46499342443,\n",
       "  542.112346692964])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_model_training(loader, q_phi, pi_theta, p_psi, p_omega, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "dda690d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved skill_samples.png\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_skills(p_omega, p_psi, s0_full, K=100):\n",
    "    \"\"\"\n",
    "    Takes as input a start state, s0.\n",
    "    Then, samples K skills randomly from the skill prior.\n",
    "    Finally, uses the TAWM to predict the terminal state of each skill.\n",
    "\n",
    "    Plots s0->sT_pred as vectors using the x-y coordinates.\n",
    "    \"\"\"\n",
    "    s0_full = torch.as_tensor(s0_full, dtype=torch.float32)\n",
    "    s0_b = s0_full.unsqueeze(0) # ensure [1, state_dim]\n",
    "\n",
    "    mu_z, std_z = p_omega(s0_b) # compute the metrics for a sampled skill                            \n",
    "    Z = mu_z.shape[-1]\n",
    "\n",
    "    # Sample K epsilons and map to skills using reparameterization trick\n",
    "    eps = torch.randn(K, Z)                  \n",
    "    z = mu_z + std_z * eps                                  \n",
    "\n",
    "    s0_expanded = s0_b.expand(K, -1) \n",
    "    # Preduct the terminal state using sampled skills and start state                      \n",
    "    sT_mu, _ = p_psi(s0_expanded, z)                        \n",
    "\n",
    "    xy0 = s0_b[..., -2:]                                      \n",
    "    xyT = sT_mu[..., -2:]                                     \n",
    "\n",
    "    x0, y0 = xy0[0].tolist()\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for i in range(K):\n",
    "        plt.plot([x0, xyT[i,0].item()], [y0, xyT[i,1].item()], alpha=0.25)\n",
    "\n",
    "    plt.scatter([x0], [y0], c=\"k\", s=60, label=\"start\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Sampled skills from prior at s0 (TAWM endpoints)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"skill_samples.png\", dpi=150)\n",
    "    print(\"Saved skill_samples.png\")\n",
    "\n",
    "\n",
    "s0_env, _ = env.reset(seed=0)                              \n",
    "obs = torch.tensor(s0_env[\"observation\"], dtype=torch.float32)\n",
    "ach = torch.tensor(s0_env[\"achieved_goal\"], dtype=torch.float32)\n",
    "\n",
    "s0_full = torch.cat([obs, ach], dim=-1)                   \n",
    "visualize_skills(p_omega, p_psi, s0_full, K=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eabf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEM Planner Parameters\n",
    "H = 40\n",
    "K = 1000 # batch size at each iteration of planning\n",
    "L = 3 # length of sequence of epsilons in the batch\n",
    "N_keep = 200 # number of sequences of skills to keep\n",
    "N_iters = 10 # number of iterations to update the diagonal gaussian\n",
    "tau = 30 # number of timesteps before replanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "5d45811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG2PI = math.log(2 * math.pi)\n",
    "\n",
    "@torch.no_grad()\n",
    "def diag_gaussian_log_p(x, mu, std):\n",
    "    # Computes the log p(x) of a diagonal gaussian\n",
    "    var = std * std\n",
    "    return (-0.5 * ((x - mu)**2 / var + 2.0 * std.log() + LOG2PI)).sum(dim=-1)\n",
    "\n",
    "class CEMPlanner:\n",
    "    def __init__(self, pi_theta, p_psi, p_omega, z_dim=Z_DIM, plan_len=L,\n",
    "                 batch_size=K, n_keep=N_keep, iters=N_iters):\n",
    "        self.pi_theta = pi_theta.eval()\n",
    "        self.p_psi = p_psi.eval()\n",
    "        self.p_omega = p_omega.eval()\n",
    "        self.z_dim = z_dim\n",
    "        self.L = plan_len\n",
    "        self.batch_size = batch_size\n",
    "        self.k = n_keep\n",
    "        self.iters = iters\n",
    "        # Keep track of the current mean and std of the diagonal Gaussian\n",
    "        self.eps_mean = torch.zeros(self.L, z_dim)\n",
    "        self.eps_std  = torch.ones (self.L, z_dim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eps_to_z_seq(self, s0, eps_seq):\n",
    "        \"\"\"\n",
    "        Given a eps_seq, convert each epsilon into a skill using the skill prior and use the TAWM to predict the state after the skill\n",
    "        Output: returns the sequence of skills and the final predicted state\n",
    "        \"\"\"\n",
    "        s = s0 # [1, state_dim]\n",
    "        z_seq = []\n",
    "        for t in range(self.L):\n",
    "            mu_w, std_w = self.p_omega(s)               \n",
    "            z_t = mu_w + std_w * eps_seq[t:t+1, :] # convert the et to zt \n",
    "            z_seq.append(z_t)\n",
    "            # roll abstract world model to get next state (use mean for planning)\n",
    "            mu_T, _ = self.p_psi(s, z_t)                \n",
    "            s = mu_T\n",
    "        z_seq = torch.cat(z_seq, dim=0)                 \n",
    "        return z_seq, s # final predicted state after L skills\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _cost_fn(self, s0, goal_xy, eps_batch):\n",
    "        \"\"\"\n",
    "        Computes the cost of each sequence of skills in the batch\n",
    "        \"\"\"\n",
    "        # eps_batch: [K, L]\n",
    "        N = eps_batch.shape[0]\n",
    "        costs = torch.empty(N)\n",
    "        # For each batch, find the final predicted state and extract the xy coordinates from the state. Then, compute the l2 distance\n",
    "        for i in range(N):\n",
    "            _, sT = self._eps_to_z_seq(s0, eps_batch[i])    \n",
    "            sT_xy = sT[..., -2:]                 \n",
    "            costs[i] = torch.linalg.norm(sT_xy[0] - goal_xy[0], ord=2)\n",
    "        return costs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def plan(self, s0, goal):\n",
    "        \"\"\"\n",
    "        Given a start state, fit a diagonal Gaussian using the N_keep best costs for N_iters\n",
    "        \"\"\"\n",
    "        mean = self.eps_mean.clone()\n",
    "        std  = self.eps_std.clone()\n",
    "\n",
    "        for _ in range(self.iters):\n",
    "            eps = mean + std * torch.randn(self.batch_size, self.L, self.z_dim) # sample K L-length sequences of epsilons from a unit Gaussian\n",
    "            costs = self._cost_fn(s0, goal, eps) # compute the costs of each sequence\n",
    "            top_k = torch.topk(-costs, self.k).indices # extract the indices of the N_keep best\n",
    "            top_k_eps = eps[top_k]                           \n",
    "            mean = top_k_eps.mean(dim=0) # take the mean and std of the epsilon sequences across the N_keep best sequences\n",
    "            std  = top_k_eps.std(dim=0) + 1e-6\n",
    "\n",
    "        final_eps = mean + std * torch.randn(self.batch_size, self.L, self.z_dim)\n",
    "        final_costs = self._cost_fn(s0, goal, final_eps)\n",
    "        best_idx = torch.argmin(final_costs)\n",
    "        best_eps = final_eps[best_idx]\n",
    "\n",
    "        self.eps_mean = torch.zeros_like(self.eps_mean)\n",
    "        self.eps_std  = torch.ones_like(self.eps_std)\n",
    "\n",
    "        # find the sequence of skills in the best sequence\n",
    "        z_seq, _ = self._eps_to_z_seq(s0, best_eps)\n",
    "        return z_seq[0:1, :], mean, std # z_seq[0:1, :]: [1, z_dim] returns the first skill in the best sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_state(obs, device):\n",
    "    base = np.asarray(obs[\"observation\"], dtype=np.float32)        # [27,]\n",
    "    ach = np.asarray(obs[\"achieved_goal\"], dtype=np.float32)      # [2,]\n",
    "    s = np.concatenate([base, ach], axis=-1)                    # [29,]\n",
    "    return torch.as_tensor(s, dtype=torch.float32, device=device).unsqueeze(0)  # [1,27]\n",
    "\n",
    "def get_goal_xy(obs, device):\n",
    "    assert isinstance(obs, dict)\n",
    "    g = np.asarray(obs[\"desired_goal\"], dtype=np.float32)          # (2,)\n",
    "    return torch.as_tensor(g, dtype=torch.float32, device=device).unsqueeze(0)  # [1,2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "planner = CEMPlanner(\n",
    "    pi_theta, p_psi, p_omega,\n",
    "    z_dim=Z_DIM, plan_len=L,\n",
    "    batch_size=K, n_keep=N_keep, iters=N_iters \n",
    ")\n",
    "\n",
    "max_steps = 2000\n",
    "obs = env.reset(seed=0)[0]\n",
    "\n",
    "state   = combine_state(obs, device)      \n",
    "goal_xy = get_goal_xy(obs, device) \n",
    "\n",
    "steps = 0\n",
    "while steps < max_steps:\n",
    "    z_1, eps_mean, eps_std = planner.plan(state, goal_xy) # z_1: [1, Z_DIM]\n",
    "\n",
    "    # Execute this skill for τ low-level steps, then replan\n",
    "    for _ in range(tau):\n",
    "        with torch.no_grad():\n",
    "            mu_a, _ = pi_theta(state, z_1) # [1, a_dim]\n",
    "            a = mu_a.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(a)\n",
    "        done  = terminated or truncated\n",
    "        state = combine_state(next_obs, device) # rebuild [1,29] by adding in the xy location\n",
    "        steps += 1\n",
    "\n",
    "        # Distance check using last two dims of the concatenated state\n",
    "        curr_xy = state[..., -2:]      \n",
    "        if torch.linalg.vector_norm(curr_xy - goal_xy[0]) < 1.0 or done:\n",
    "            break\n",
    "\n",
    "    curr_xy = state[..., -2:] \n",
    "    if torch.linalg.vector_norm(curr_xy - goal_xy[0]) < 1.0 or done:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oposm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
