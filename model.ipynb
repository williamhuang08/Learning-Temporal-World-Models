{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edb0f36",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Model-Based Temporal Abstraction involves simultaneuously learning\n",
    "1) skill-conditioned low-level policy\n",
    "2) skill-conditioned temporally abstract world model\n",
    "\n",
    "Notation\n",
    "- skill-conditioned low-level policy: $\\pi_{\\theta}(a_t|s_t, z)$\n",
    "    - $\\theta$ are parameters\n",
    "    - $a_t \\in A$ is current action selected by agent\n",
    "    - $s_t \\in S$ is current state\n",
    "    - $z \\in Z$ is abstract skill variable that encodes a skill\n",
    "\n",
    "- skill-conditioned temporally abstract world model (TAWM): $p_{\\psi}(s'|s,z)$ (models distribution of states agent is in after skill $z$)\n",
    "    - $\\psi$ parameters\n",
    "    - $z$ is current skill\n",
    "\n",
    "Note: low-level policy and TAWM not trained on rewards, reward function is provided later for planning with the learned skills \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "c3e4c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages for the whole script\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Independent, kl_divergence\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import minari\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4bb13",
   "metadata": {},
   "source": [
    "### Learning $\\pi_{\\theta}$ and $p_{\\psi}$\n",
    "\n",
    "Learning $\\pi_{\\theta}$ and $p_{\\psi}$ requires treating skills as latent variables and optimizing the ELBO\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\psi,\\phi,\\omega)\n",
    "= \\mathbb{E}_{\\tau_T \\sim \\mathcal{D}}\\!\\left[\n",
    "  \\mathbb{E}_{q_\\phi(z\\,|\\,\\tau_T)}\\!\\left[\n",
    "    \\log \\pi_\\theta(\\bar{a}\\,|\\,\\bar{s}, z)\n",
    "    + \\log p_\\psi(s_T \\,|\\, s_0, z)\n",
    "  \\right]\n",
    "  - D_{\\mathrm{KL}}\\!\\left(q_\\phi(z\\,|\\,\\tau_T)\\,\\|\\,p_\\omega(z\\,|\\,s_0)\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "where $\\tau_T$ is a T-length subtrajectory sampled from the offline dataset $\\mathcal{D}$, $\\bar{s}$ and $\\bar{a}$ are state and action sequences of $\\tau_T$, $q_{\\psi}$ is a posterior over $z$ given $\\tau_T$, and $p_{\\omega}$ is a prior of $z$ given $s_0$.\n",
    "\n",
    "The first term is the log-likelihood of demonstrator actions. This ensures that the low-level policy can reproduce a demonstrator's action sequence given a skill. This forces $z$ to encode control-relevant information.\n",
    "\n",
    "The second term is the log-likelihood of long-term state transitions. This term ensures that we learn relationships between $z$ to what possible $s_T$ could result from. the skill.\n",
    "\n",
    "Finally, the last term is the KL divergence between skill posterior and prior (encourages compression of skills). Therefore, maximizing this ELBO makes skills $z$ explain the data and keeps the KL divergence small. This ensures that the skill is start-state predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "695e8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the paper, each layer contains 256 neurons\n",
    "NUM_NEURONS = 256\n",
    "# The dimension of the abstract skill variable, z\n",
    "Z_DIM = 256\n",
    "\n",
    "# Skill Posterior, q_phi\n",
    "class SkillPosterior(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: sequence of skills and actions\n",
    "    Output: mean and std over z\n",
    "\n",
    "    1. Linear layer w/ ReLU activation for the state sequence\n",
    "    2. Single-layer bidirectional GRU for embedded states and action sequence (concatenated)\n",
    "    3. Extract mean and std of layer 2's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bi_gru = nn.GRU(input_size=NUM_NEURONS+self.action_dim, hidden_size= NUM_NEURONS//2, bidirectional=True, batch_first=True)\n",
    "        self.mean = MeanNetwork(Z_DIM)\n",
    "        self.std = StandardDeviationNetwork(Z_DIM)\n",
    "\n",
    "    def forward(self, state_sequence, action_sequence):\n",
    "        embedded_states = self.relu(self.fc1(state_sequence))\n",
    "        concatenated = torch.cat([embedded_states, action_sequence], dim=-1)\n",
    "        x, _ = self.bi_gru(concatenated) # [B, T, NUM_NEURONS]\n",
    "        seq_emb = x.mean(dim=1) # [B, NUM_NEURONS]\n",
    "        mean = self.mean.forward(seq_emb)\n",
    "        std = self.std.forward(seq_emb)\n",
    "        return mean, std\n",
    "\n",
    "# Low-Level Skill-Conditioned Policy, pi_theta\n",
    "class SkillPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: Current state and a skill, z\n",
    "    Output: mean and std over a\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for the state and abstract skill (concatenated)\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(self.action_dim)\n",
    "        self.std = StandardDeviationNetwork(self.action_dim)\n",
    "    \n",
    "    def forward(self, state, z):\n",
    "        c = torch.cat([state, z], dim=-1)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "        \n",
    "\n",
    "# Temporally-Abstract World Model, p_psi\n",
    "class TAWM(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: initial state, along with the abstract skill\n",
    "    Output: mean and std over terminal state\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for initial state and abstract skill (concatenated)\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(self.state_dim)\n",
    "        self.std = StandardDeviationNetwork(self.state_dim)\n",
    "    \n",
    "    def forward(self, input_state, z):\n",
    "        c = torch.cat([input_state, z], dim=-1)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "\n",
    "# Skill Prior, p_omega\n",
    "class SkillPrior(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: Initial state, s0, in the trajectory\n",
    "    Output: mean and std over the abstract skill, z\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for the initial state\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(Z_DIM)\n",
    "        self.std = StandardDeviationNetwork(Z_DIM)\n",
    "    \n",
    "    def forward(self, input_state):\n",
    "        x = self.relu(self.fc1(input_state))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "\n",
    "class MeanNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: tensor to calculate mean\n",
    "    Output: mean of input w/ dimension out_dim\n",
    "\n",
    "    1. 2-layer network w/ ReLU activation for the first layer\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class StandardDeviationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: tensor to calculate std\n",
    "    Output: std of input w/ dimension out_dim\n",
    "\n",
    "    Note: the standard deviation is lower and upper bounded at 0.05 and 2.0\n",
    "    - if std is 0, then log(std) -> inf\n",
    "    - if std is large, then can affect training\n",
    "\n",
    "    1. 2-layer linear network with ReLU activation after first layer and softplus after second\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim, min_std=0.05, max_std=2.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.min_std = min_std\n",
    "        self.max_std = max_std\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        std = self.softplus(x) + self.min_std  # lower bound\n",
    "        std = torch.clamp(std, max=self.max_std)\n",
    "        return std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4f8a0",
   "metadata": {},
   "source": [
    "#### The Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "Since calculating the true posterior of $z$ given $\\tau_T$ is intractable, we infer $q_{\\psi}(z|\\tau_T)$.\n",
    "\n",
    "1. E-Step:\n",
    "- Update $\\psi$ w/gradient descent so that KL divergence between $q_\\psi$ and true posterior is minimized\n",
    "\n",
    "2. M-Step:\n",
    "- Fixing $q_{\\psi}$, update ($\\theta, \\psi, \\omega$) s.t. ELBO is maximized using gradient ascent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c61868",
   "metadata": {},
   "source": [
    "##### E-Step (Update $\\psi$)\n",
    "\n",
    "In this step, we want to minimize \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{T}_T\\sim\\mathcal{D}}\n",
    "\\Bigg[\n",
    "\\mathbb{E}_{z\\sim q_\\phi}\n",
    "\\bigg[\n",
    "\\log \\frac{q_\\phi\\!\\left(z\\mid \\bar{s},\\bar{a}\\right)}\n",
    "{\\pi_\\theta\\!\\left(\\bar{a}\\mid \\bar{s}, z\\right)\\,p_\\omega\\!\\left(z\\mid s_0\\right)}\n",
    "\\bigg]\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "Equivalently, we want to minimize $\\mathcal{KL}(q_{\\psi}||p)$ where $p$, the true posterior is \n",
    "\n",
    "$$\n",
    "p(z \\mid \\bar{s}, \\bar{a}) = \\frac{1}{\\eta}\\,\\pi_\\theta(\\bar{a}\\mid \\bar{s}, z)\\,p_\\omega(z\\mid s_0).\n",
    "$$\n",
    "\n",
    "\n",
    "##### M-Step (Update $\\theta$, $\\psi$, $\\omega$)\n",
    "\n",
    "In this step, we want to update $\\theta$, $\\psi$, and $\\omega$ using gradient ascent to maximize the ELBO from above.\n",
    "\n",
    "Both steps are trained using an Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae831ff",
   "metadata": {},
   "source": [
    "##### Dataset 1: AntMaze Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "c4a079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the AntMaze dataset in Minari format\n",
    "ant_maze_dataset = minari.load_dataset('D4RL/antmaze/medium-play-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "1cd992f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "dict_keys(['achieved_goal', 'desired_goal', 'observation'])\n",
      "(1001, 27)\n",
      "(1001, 2)\n"
     ]
    }
   ],
   "source": [
    "print(ant_maze_dataset[0].actions.shape)\n",
    "print(ant_maze_dataset[0].observations.keys())\n",
    "print(ant_maze_dataset[0].observations[\"observation\"].shape)\n",
    "print(ant_maze_dataset[0].observations[\"achieved_goal\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "93eff05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, the number of subtrajectories per batch (from paper)\n",
    "B = 100\n",
    "\n",
    "# T, the length of each subtrajectory (from paper)\n",
    "T = 9\n",
    "\n",
    "# AntMaze state and action dims (from Minari)\n",
    "state_dim = 29\n",
    "action_dim = 8\n",
    "\n",
    "# Initialize the models\n",
    "q_phi = SkillPosterior(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "pi_theta = SkillPolicy(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "p_psi = TAWM(state_dim=state_dim).to(device)\n",
    "p_omega = SkillPrior(state_dim=state_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "d3c71a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Input: LoadedMinari dataset and the length of the number of actions in each subtrajectory (T)\n",
    "    Output: Dictionary with keys \"s0, state_sequence, action_sequence, and sT\"\n",
    "\n",
    "    Finds all episodes that have at least T actions. Then, for each of those episodes, creates a sliding window to create subtrajectories w/ T actions\n",
    "    \"\"\"\n",
    "    def __init__(self, ant_maze_dataset, T):\n",
    "        self.T = T\n",
    "        self.subtrajectories = []\n",
    "\n",
    "        for ep in ant_maze_dataset.iterate_episodes():\n",
    "            s = ep.observations[\"observation\"]\n",
    "            a = ep.actions\n",
    "            l = len(s)\n",
    "            if l < T + 1:\n",
    "                continue\n",
    "            \n",
    "            # Consider skipping timesteps so that subtrajectories don't overlap that much\n",
    "            stride = 3\n",
    "            for t in range(0, l - T, stride):\n",
    "                obs = ep.observations[\"observation\"][t:t+T+1] # (27,)\n",
    "                ach = ep.observations[\"achieved_goal\"][t:t+T+1]  # (2,)\n",
    "                state_sequence_extended = np.concatenate([obs, ach], axis=-1)\n",
    "                state_sequence = state_sequence_extended[:-1]\n",
    "                s0 = state_sequence[0]\n",
    "                action_sequence = a[t: t + T]\n",
    "                sT = state_sequence_extended[-1]\n",
    "                self.subtrajectories.append((s0, state_sequence, action_sequence, sT))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subtrajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s0, state_sequence, action_sequence, sT = self.subtrajectories[idx]\n",
    "\n",
    "        return {\n",
    "            \"s0\": torch.as_tensor(s0, dtype=torch.float32),\n",
    "            \"state_sequence\": torch.as_tensor(state_sequence, dtype=torch.float32),\n",
    "            \"action_sequence\": torch.as_tensor(action_sequence, dtype=torch.float32),\n",
    "            \"sT\": torch.as_tensor(sT, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "def collate(batch):\n",
    "    # Vertically stacks each of the components of the subtrajectories such that the first dimension is the batch\n",
    "    return {\n",
    "        \"s0\": torch.stack([b[\"s0\"] for b in batch], 0),\n",
    "        \"state_sequence\": torch.stack([b[\"state_sequence\"] for b in batch], 0),\n",
    "        \"action_sequence\": torch.stack([b[\"action_sequence\"] for b in batch], 0),\n",
    "        \"sT\": torch.stack([b[\"sT\"] for b in batch], 0)\n",
    "    }\n",
    "\n",
    "# Create the dictionary of subtrajectories\n",
    "dataset = TrainingDataset(ant_maze_dataset, T)\n",
    "\n",
    "# Iterator w/ groups of subtrajectories from the dataset of size B \n",
    "loader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "5fcad857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_e_loss(batch):\n",
    "    s0  = batch[\"s0\"]               \n",
    "    S   = batch[\"state_sequence\"]   \n",
    "    A   = batch[\"action_sequence\"]  \n",
    "    Bsz, T, _ = S.shape\n",
    "\n",
    "    # Sampling z using the reparameterization trick where z = mu(tau) + std(tau) * epsilon\n",
    "    mu_q, std_q = q_phi(S, A) # [B, z_dim]\n",
    "    eps = torch.randn_like(mu_q)\n",
    "    z = (mu_q + std_q * eps)      \n",
    "\n",
    "    # Freeze the weights of the low-level skill-conditioned policy, pi_theta\n",
    "    with torch.no_grad():\n",
    "        z_bt = z.unsqueeze(1).expand(Bsz, T, -1) # [B, T, z_dim]\n",
    "        # Flatten time dimension since pi_theta does not the expect extra dim ([B * T, s_dim])\n",
    "        Sf = S.reshape(Bsz*T, -1)\n",
    "        Zf = z_bt.reshape(Bsz*T, -1)\n",
    "        mu_pi, std_pi = pi_theta(Sf, Zf)     \n",
    "        # Restore time dimension ([B, T, a_dim])\n",
    "        mu_pi  = mu_pi.view(Bsz, T, -1)\n",
    "        std_pi = std_pi.view(Bsz, T, -1)\n",
    "        # Build a MVN over independent actions at each timestep of each batch and sum log_probs across action_dim\n",
    "        pi_dist = Independent(Normal(mu_pi, std_pi), 1)\n",
    "        # Compute the log probability of observed actions\n",
    "        log_pi = pi_dist.log_prob(A) # ([B, T])    \n",
    "\n",
    "    # Freeze the weights of the skill prior, p_omega\n",
    "    with torch.no_grad():\n",
    "        # Find the distribution of the abstract skill given start states (mu_pr & std_pr: [B, z_dim])\n",
    "        mu_pr, std_pr = p_omega(s0)  \n",
    "        # Build a MVN over independent skills at each timestep of each batch and sum log_probs across z_dim\n",
    "        prior_dist = Independent(Normal(mu_pr, std_pr), 1)\n",
    "        # Compute the log-probability over the sampled skills using the prior\n",
    "        log_p_omega = prior_dist.log_prob(z)   \n",
    "\n",
    "    post_dist = Independent(Normal(mu_q, std_q), 1)\n",
    "    # Compute the log-probability over the sampled skills using the inferred posterior\n",
    "    log_q = post_dist.log_prob(z)              \n",
    "\n",
    "    # Calculate the E-objective\n",
    "    e_obj = (log_pi.sum(dim=1) + log_p_omega - log_q)\n",
    "    e_loss = -e_obj.mean() # minimize the negative objective\n",
    "    return e_loss\n",
    "\n",
    "\n",
    "\n",
    "def compute_m_loss(batch):\n",
    "    s0  = batch[\"s0\"]               \n",
    "    S   = batch[\"state_sequence\"]   \n",
    "    A   = batch[\"action_sequence\"] \n",
    "    sT  = batch[\"sT\"]               \n",
    "    Bsz, T, Sdim = S.shape\n",
    "\n",
    "    # Freeze the weights of the inferred posterior, only update omega, psi, and theta\n",
    "    with torch.no_grad():\n",
    "        mu_q, std_q = q_phi(S, A)        # [B, z_dim]\n",
    "        eps = torch.randn_like(mu_q)\n",
    "        # same reparameterization trick as E-loss\n",
    "        z = (mu_q + std_q * eps)  \n",
    "\n",
    "    z_bt = z.unsqueeze(1).expand(Bsz, T, -1)\n",
    "    # Flatten time dimension since pi_theta does not the expect extra dim ([B * T, s_dim])\n",
    "    Sf = S.reshape(Bsz*T, -1)\n",
    "    Zf = z_bt.reshape(Bsz*T, -1)\n",
    "    mu_pi, std_pi = pi_theta(Sf, Zf)    \n",
    "    # Restore time dimension ([B, T, a_dim])     \n",
    "    mu_pi = mu_pi.reshape(Bsz, T, -1)\n",
    "    std_pi = std_pi.reshape(Bsz, T, -1)\n",
    "    # Build a MVN over independent actions at each timestep of each batch and sum log_probs across action_dim\n",
    "    pi_dist = Independent(Normal(mu_pi, std_pi), 1)\n",
    "    log_pi = pi_dist.log_prob(A)               \n",
    "\n",
    "    # Zs: [B, T, z_dim]\n",
    "    mu_T, std_T = p_psi(s0, z)\n",
    "    # Build a MVN over independent terminal states at each timestep of each batch and sum log_probs across state_dim     \n",
    "    ppsi_dist = Independent(Normal(mu_T, std_T), 1)\n",
    "    # Compute the log-probability over the observed terminal states using the TAWM\n",
    "    log_p_psi = ppsi_dist.log_prob(sT)          \n",
    "\n",
    "    # Find the distribution of the abstract skill given start states (mu_pr & std_pr: [B, z_dim])\n",
    "    mu_pr, std_pr = p_omega(s0)                \n",
    "    # Build a MVN over independent skills at each timestep of each batch and sum log_probs across z_dim\n",
    "    prior_dist = Independent(Normal(mu_pr, std_pr), 1)\n",
    "    # Compute the log-probability over the sampled skills using the prior\n",
    "    log_p_omega = prior_dist.log_prob(z)   \n",
    "\n",
    "    # sum to find join log-likelihood of the whole sequence\n",
    "    obj = log_pi.sum(dim=1) + log_p_psi + log_p_omega   \n",
    "    m_loss = -obj.mean()\n",
    "    return m_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def skill_model_training(\n",
    "    loader,\n",
    "    q_phi, pi_theta, p_psi, p_omega,\n",
    "    e_lr=5e-5, m_lr=5e-5,\n",
    "    epochs=50,\n",
    "    e_steps=1, m_steps=1,\n",
    "    grad_clip=1.0 # prevents runaway gradients\n",
    "):\n",
    "    # Skill model training setup\n",
    "    q_phi.to(device)\n",
    "    pi_theta.to(device)\n",
    "    p_psi.to(device)\n",
    "    p_omega.to(device)\n",
    "\n",
    "    e_optimizer = torch.optim.Adam(q_phi.parameters(), lr=e_lr)\n",
    "    m_optimizer = torch.optim.Adam(list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), lr=m_lr)\n",
    "\n",
    "    e_curve = []   \n",
    "    m_curve = []   \n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Running e_loss, m_loss, and batches in current epoch\n",
    "        e_running, m_running, nb = 0.0, 0.0, 0\n",
    "\n",
    "        for batch in loader:\n",
    "            # Rebuilds dictionary but moves tensors to the device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            nb += 1\n",
    "\n",
    "            # In E-step, train the posterior while freezing other parameters\n",
    "            q_phi.train()\n",
    "            pi_theta.eval()\n",
    "            p_psi.eval()\n",
    "            p_omega.eval()\n",
    "\n",
    "            for p in q_phi.parameters(): \n",
    "                p.requires_grad_(True)\n",
    "            for m in (pi_theta, p_psi, p_omega):\n",
    "                for p in m.parameters(): \n",
    "                    p.requires_grad_(False)\n",
    "\n",
    "            # For the e-step, \n",
    "            for _ in range(e_steps):\n",
    "                # Resent gradients\n",
    "                e_optimizer.zero_grad(set_to_none=True)\n",
    "                e_loss = compute_e_loss(batch)\n",
    "                e_loss.backward() \n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(q_phi.parameters(), grad_clip)\n",
    "                e_optimizer.step() # Update the parameters of the posterior\n",
    "\n",
    "            e_running += e_loss.item()\n",
    "\n",
    "            # Freeze posterior weights, update all other weights\n",
    "            q_phi.eval()\n",
    "            pi_theta.train()\n",
    "            p_psi.train()\n",
    "            p_omega.train()\n",
    "\n",
    "            for p in q_phi.parameters(): \n",
    "                p.requires_grad_(False)\n",
    "            for m in (pi_theta, p_psi, p_omega):\n",
    "                for p in m.parameters(): \n",
    "                    p.requires_grad_(True)\n",
    "\n",
    "            for _ in range(m_steps):\n",
    "                # Reset gradients\n",
    "                m_optimizer.zero_grad(set_to_none=True)\n",
    "                m_loss = compute_m_loss(batch)\n",
    "                m_loss.backward()\n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), grad_clip)\n",
    "                m_optimizer.step() # Update theta, psi, and omega\n",
    "\n",
    "            m_running += m_loss.item()\n",
    "\n",
    "        # Calculate the average losses over all the batches in the epoch\n",
    "        e_epoch = e_running / max(1, nb)\n",
    "        m_epoch = m_running / max(1, nb)\n",
    "        e_curve.append(e_epoch)\n",
    "        m_curve.append(m_epoch)\n",
    "        print(f\"[Epoch {epoch:03d}/{epochs}]  E: {e_epoch:.4f}   M: {m_epoch:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(e_curve, label=\"E-loss\")\n",
    "    plt.plot(m_curve, label=\"M-loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"EM training losses\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return e_curve, m_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f2da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001/25]  E: 47.1526   M: 612.1870\n",
      "[Epoch 002/25]  E: 31.5192   M: 588.5160\n"
     ]
    }
   ],
   "source": [
    "skill_model_training(loader, q_phi, pi_theta, p_psi, p_omega, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4c883",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'expand'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[463], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m start_state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 35\u001b[0m \u001b[43mvisualize_skills\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_omega\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_psi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/oposm/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[463], line 15\u001b[0m, in \u001b[0;36mvisualize_skills\u001b[0;34m(p_omega, p_psi, s0, K, fname)\u001b[0m\n\u001b[1;32m     12\u001b[0m mu_z, std_z \u001b[38;5;241m=\u001b[39m p_omega(full_state)                        \u001b[38;5;66;03m# [1, z_dim]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m z \u001b[38;5;241m=\u001b[39m mu_z \u001b[38;5;241m+\u001b[39m std_z \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(K, mu_z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m s0_rep \u001b[38;5;241m=\u001b[39m \u001b[43ms0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m(K, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                        \u001b[38;5;66;03m# [K, s_dim]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m sT_mu, _ \u001b[38;5;241m=\u001b[39m p_psi(s0_rep, z)                      \u001b[38;5;66;03m# [K, s_dim] (mean next state)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m xy0 \u001b[38;5;241m=\u001b[39m state_to_xy(s0_rep)                        \u001b[38;5;66;03m# [K, 2]\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'expand'"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_skills(p_omega, p_psi, s0, K=100):\n",
    "    \"\"\"\n",
    "    Takes as input a start state, s0.\n",
    "    Then, samples K skills randomly from the skill prior.\n",
    "    Finally, uses the TAWM to predict the terminal state of each skill.\n",
    "\n",
    "    Plots s0->sT_pred as vectors using the x-y coordinates.\n",
    "    \"\"\"\n",
    "    mu_z, std_z = p_omega(s0) # compute the distribution of the skill from s0 using prior                    \n",
    "    z = mu_z + std_z * torch.randn(K, mu_z.size(-1)) # sample k skills using the reparamterization trick\n",
    "\n",
    "    s0_expanded = s0.expand(K, -1)                        \n",
    "    sT_mu, _ = p_psi(s0_expanded, z)                      \n",
    "    # Extract the xy coordinates from s0 and sT\n",
    "    xy0 = s0[:,:,:2]                    \n",
    "    xyT = sT_mu[:,:,:2]                      \n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for i in range(K):\n",
    "        plt.plot([xy0[i,0].cpu(), xyT[i,0].cpu()],\n",
    "                 [xy0[i,1].cpu(), xyT[i,1].cpu()],\n",
    "                 alpha=0.25)\n",
    "    plt.scatter(xy0[0,0].cpu(), xy0[0,1].cpu(), c=\"k\", s=60, label=\"start\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Sampled skills: skills from prior starting at s0\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"skill_samples.png\", dpi=150)\n",
    "    print(\"Saved skill_samples.png\")\n",
    "\n",
    "s0 = env.reset()[0]\n",
    "obs = s0[\"observation\"]\n",
    "ach = s0[\"achieved_goal\"]\n",
    "s0_full = np.concatenate([obs, ach], axis=-1)\n",
    "visualize_skills(p_omega, p_psi, s0_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eabf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEM Planner Parameters\n",
    "H = 40\n",
    "K = 1000\n",
    "L = 3\n",
    "N_keep = 200\n",
    "N_iters = 10\n",
    "tau = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG2PI = 1.8378770664093453\n",
    "\n",
    "@torch.no_grad()\n",
    "def diag_normal_log_prob(x, mu, std):\n",
    "    var = std * std\n",
    "    return (-0.5 * ((x - mu)**2 / var + 2.0 * std.log() + LOG2PI)).sum(dim=-1)\n",
    "\n",
    "class CEMPlanner:\n",
    "    def __init__(self, pi_theta, p_psi, p_omega, z_dim, plan_len,\n",
    "                 pop_size=512, elite_frac=0.1, iters=10, l2_pen=0.0,\n",
    "                 max_eps=None, device=\"cuda\", state_to_xy=state_to_xy):\n",
    "        self.pi_theta = pi_theta.eval()\n",
    "        self.p_psi = p_psi.eval()\n",
    "        self.p_omega = p_omega.eval()\n",
    "        self.z_dim = z_dim\n",
    "        self.L = plan_len\n",
    "        self.pop = pop_size\n",
    "        self.k = max(1, int(pop_size*elite_frac))\n",
    "        self.iters = iters\n",
    "        self.l2 = l2_pen\n",
    "        self.max_eps = max_eps\n",
    "        self.device = device\n",
    "        # MPC warm-start buffers\n",
    "        self.eps_mean = torch.zeros(self.L, z_dim, device=device)\n",
    "        self.eps_std  = torch.ones (self.L, z_dim, device=device)\n",
    "        self.state_to_xy = state_to_xy\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eps_to_z_seq(self, s0, eps_seq):\n",
    "        \"\"\"\n",
    "        Convert an ε-sequence (L, z_dim) into a z-sequence using the *state-dependent* prior.\n",
    "        We update s iteratively with the world model mean to get μ_ω(s_t), σ_ω(s_t).\n",
    "        \"\"\"\n",
    "        B = 1\n",
    "        s = s0  # [1, s_dim]\n",
    "        z_seq = []\n",
    "        for t in range(self.L):\n",
    "            mu_w, std_w = self.p_omega(s)               # [1, z_dim]\n",
    "            z_t = mu_w + std_w * eps_seq[t:t+1, :]      # [1, z_dim]\n",
    "            z_seq.append(z_t)\n",
    "            # roll abstract world model to get next state (use mean for planning)\n",
    "            mu_T, _ = self.p_psi(s, z_t)                # [1, s_dim]\n",
    "            s = mu_T\n",
    "        z_seq = torch.cat(z_seq, dim=0)                 # [L, z_dim]\n",
    "        return z_seq, s                                  # final predicted state after L skills\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _cost_fn(self, s0, goal_xy, eps_batch):\n",
    "        N = eps_batch.shape[0]\n",
    "        costs = torch.empty(N, device=self.device)\n",
    "        for i in range(N):\n",
    "            _, sL = self._eps_to_z_seq(s0, eps_batch[i])     # sL:[1,27]\n",
    "            sL_xy = self.state_to_xy(sL)                     # -> [1,2]\n",
    "            costs[i] = torch.linalg.norm(sL_xy[0] - goal_xy[0], ord=2)\n",
    "        if self.l2 > 0:\n",
    "            costs += self.l2 * eps_batch.pow(2).mean(dim=(1,2))\n",
    "        return costs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def plan(self, s0, goal):\n",
    "        \"\"\"\n",
    "        s0: [1, s_dim], goal: [1, s_dim]\n",
    "        Returns:\n",
    "          z_first: [1, z_dim]  (first skill to execute)\n",
    "          eps_mean, eps_std: updated MPC warm-start stats\n",
    "        \"\"\"\n",
    "        mean = self.eps_mean.clone()\n",
    "        std  = self.eps_std.clone()\n",
    "\n",
    "        for _ in range(self.iters):\n",
    "            eps = mean + std * torch.randn(self.pop, self.L, self.z_dim, device=self.device)\n",
    "            if self.max_eps is not None:\n",
    "                eps.clamp_(-self.max_eps, self.max_eps)\n",
    "            costs = self._cost_fn(s0, goal, eps)  # [N]\n",
    "            elite_idx = torch.topk(-costs, self.k).indices  # smaller cost = better\n",
    "            elite = eps[elite_idx]                           # [k, L, z]\n",
    "            mean = elite.mean(dim=0)\n",
    "            std  = elite.std(dim=0) + 1e-6\n",
    "\n",
    "        # save warm-start (shift left for MPC: keep future skills)\n",
    "        self.eps_mean = torch.cat([mean[1:], torch.zeros(1, self.z_dim, device=self.device)], dim=0)\n",
    "        self.eps_std  = torch.cat([std [1:], torch.ones (1, self.z_dim, device=self.device)], dim=0)\n",
    "\n",
    "        # return the *first* skill in Z-space to execute\n",
    "        z_seq, _ = self._eps_to_z_seq(s0, mean)\n",
    "        return z_seq[0:1, :], mean, std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "a9a2efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- helpers (use these everywhere) -----------------------------------------\n",
    "def to_state_tensor(obs, device):\n",
    "    # NN state is the 27-d vector\n",
    "    if isinstance(obs, dict): arr = obs[\"observation\"]\n",
    "    else:                     arr = obs\n",
    "    return torch.as_tensor(arr, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "def get_xy(obs):\n",
    "    assert isinstance(obs, dict), \"AntMaze obs is a dict\"\n",
    "    return torch.as_tensor(obs[\"achieved_goal\"], dtype=torch.float32)\n",
    "\n",
    "def get_goal_xy(obs):\n",
    "    assert isinstance(obs, dict)\n",
    "    return torch.as_tensor(obs[\"desired_goal\"], dtype=torch.float32)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "planner = CEMPlanner(pi_theta, p_psi, p_omega,\n",
    "                     z_dim=Z_DIM, plan_len=3,\n",
    "                     pop_size=512, elite_frac=0.1, iters=8,\n",
    "                     l2_pen=0.0, max_eps=3.0, device=device, state_to_xy=state_to_xy)\n",
    "\n",
    "H_low = 10\n",
    "max_steps = 2000\n",
    "\n",
    "obs = env.reset()[0]\n",
    "state    = to_state_tensor(obs, device)      # [1, 27]\n",
    "goal_xy  = get_goal_xy(obs).to(device).unsqueeze(0)   # [1, 2]\n",
    "\n",
    "steps = 0\n",
    "while steps < max_steps:\n",
    "    # pass goal_xy to planner (make sure planner’s cost uses XY, not state[:2])\n",
    "    z_1, eps_mean, eps_std = planner.plan(state, goal_xy)    # z_1: [1, Z_DIM]\n",
    "\n",
    "    for _ in range(H_low):\n",
    "        with torch.no_grad():\n",
    "            mu_a, _ = pi_theta(state, z_1)                   # [1, a_dim]\n",
    "            a = mu_a\n",
    "\n",
    "        step_out = env.step(a.squeeze(0).cpu().numpy())\n",
    "        if len(step_out) == 5:   # Gymnasium\n",
    "            next_obs, reward, terminated, truncated, info = step_out\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            next_obs, reward, done, info = step_out\n",
    "\n",
    "        state   = to_state_tensor(next_obs, device)\n",
    "        steps  += 1\n",
    "\n",
    "        # ---- distance check now uses true XY from the dict ----\n",
    "        curr_xy = get_xy(next_obs).to(device)                # [2]\n",
    "        if torch.linalg.vector_norm(curr_xy - goal_xy[0]) < 1.0 or done:\n",
    "            break\n",
    "\n",
    "    curr_xy = get_xy(next_obs).to(device)\n",
    "    if torch.linalg.vector_norm(curr_xy - goal_xy[0]) < 1.0 or done:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d24297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oposm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
