{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edb0f36",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Model-Based Temporal Abstraction involves simultaneuously learning\n",
    "1) skill-conditioned low-level policy\n",
    "2) skill-conditioned temporally abstract world model\n",
    "\n",
    "Notation\n",
    "- skill-conditioned low-level policy: $\\pi_{\\theta}(a_t|s_t, z)$\n",
    "    - $\\theta$ are parameters\n",
    "    - $a_t \\in A$ is current action selected by agent\n",
    "    - $s_t \\in S$ is current state\n",
    "    - $z \\in Z$ is abstract skill variable that encodes a skill\n",
    "\n",
    "- skill-conditioned temporally abstract world model (TAWM): $p_{\\psi}(s'|s,z)$ (models distribution of states agent is in after skill $z$)\n",
    "    - $\\psi$ parameters\n",
    "    - $z$ is current skill\n",
    "\n",
    "Note: low-level policy and TAWM not trained on rewards, reward function is provided later for planning with the learned skills \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4bb13",
   "metadata": {},
   "source": [
    "### Learning $\\pi_{\\theta}$ and $p_{\\psi}$\n",
    "\n",
    "Learning $\\pi_{\\theta}$ and $p_{\\psi}$ requires treating skills as latent variables and optimizing the ELBO\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\psi,\\phi,\\omega)\n",
    "= \\mathbb{E}_{\\tau_T \\sim \\mathcal{D}}\\!\\left[\n",
    "  \\mathbb{E}_{q_\\phi(z\\,|\\,\\tau_T)}\\!\\left[\n",
    "    \\log \\pi_\\theta(\\bar{a}\\,|\\,\\bar{s}, z)\n",
    "    + \\log p_\\psi(s_T \\,|\\, s_0, z)\n",
    "  \\right]\n",
    "  - D_{\\mathrm{KL}}\\!\\left(q_\\phi(z\\,|\\,\\tau_T)\\,\\|\\,p_\\omega(z\\,|\\,s_0)\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "where $\\tau_T$ is a T-length subtrajectory sampled from the offline dataset $\\mathcal{D}$, $\\bar{s}$ and $\\bar{a}$ are state and action sequences of $\\tau_T$, $q_{\\psi}$ is a posterior over $z$ given $\\tau_T$, and $p_{\\omega}$ is a prior of $z$ given $s_0$.\n",
    "\n",
    "The first term is the log-likelihood of demonstrator actions. This ensures that the low-level policy can reproduce a demonstrator's action sequence given a skill. This forces $z$ to encode control-relevant information.\n",
    "\n",
    "The second term is the log-likelihood of long-term state transitions. This term ensures that we learn relationships between $z$ to what possible $s_T$ could result from. the skill.\n",
    "\n",
    "Finally, the last term is the KL divergence between skill posterior and prior (encourages compression of skills). Therefore, maximizing this ELBO makes skills $z$ explain the data and keeps the KL divergence small. This ensures that the skill is start-state predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4f8a0",
   "metadata": {},
   "source": [
    "#### The Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "Since calculating the true posterior of $z$ given $\\tau_T$ is intractable, we infer $q_{\\psi}(z|\\tau_T)$.\n",
    "\n",
    "1. E-Step:\n",
    "- Update $\\psi$ w/gradient descent so that KL divergence between $q_\\psi$ and true posterior is minimized\n",
    "\n",
    "2. M-Step:\n",
    "- Fixing $q_{\\psi}$, update ($\\theta, \\psi, \\omega$) s.t. ELBO is maximized using gradient ascent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbcc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c61868",
   "metadata": {},
   "source": [
    "##### E-Step (Update $\\psi$)\n",
    "\n",
    "In this step, we want to minimize \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{T}_T\\sim\\mathcal{D}}\n",
    "\\Bigg[\n",
    "\\mathbb{E}_{z\\sim q_\\phi}\n",
    "\\bigg[\n",
    "\\log \\frac{q_\\phi\\!\\left(z\\mid \\bar{s},\\bar{a}\\right)}\n",
    "{\\pi_\\theta\\!\\left(\\bar{a}\\mid \\bar{s}, z\\right)\\,p_\\omega\\!\\left(z\\mid s_0\\right)}\n",
    "\\bigg]\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "Equivalently, we want to minimize $\\mathcal{KL}(q_{\\psi}||p)$ where $p$, the true posterior is \n",
    "\n",
    "$$\n",
    "p(z \\mid \\bar{s}, \\bar{a}) = \\frac{1}{\\eta}\\,\\pi_\\theta(\\bar{a}\\mid \\bar{s}, z)\\,p_\\omega(z\\mid s_0).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e2ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEURONS = 256\n",
    "Z_DIM = 256\n",
    "\n",
    "# Skill Posterior, q_psi\n",
    "class SkillPosterior(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bi_gru = nn.GRU(input_size=NUM_NEURONS+self.action_dim, hidden_size= NUM_NEURONS/2, bidirectional=True, batch_first=True)\n",
    "        self.mean = MeanNetwork()\n",
    "        self.std = StandardDeviationNetwork()\n",
    "\n",
    "    def forward(self, state_sequence, action_sequence):\n",
    "        embedded_states = self.relu(self.fc1(state_sequence))\n",
    "        concatenated = torch.cat([embedded_states, action_sequence], dim=0)\n",
    "        x, _ = self.bi_gru(concatenated)\n",
    "        mean = MeanNetwork.forward(x)\n",
    "        std = StandardDeviationNetwork.forward(x)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "# Low-Level Skill-Conditioned Policy, pi_theta\n",
    "class SkillPolicy(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.Relu()\n",
    "        self.mean = MeanNetwork()\n",
    "        self.std = StandardDeviationNetwork()\n",
    "    \n",
    "    def forward(self, state, z):\n",
    "        c = torch.cat([state, z], dim=0)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "        \n",
    "\n",
    "# Temporally-Abstract World Model, p_psi\n",
    "class TAWM(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "# Skill Prior, p_omega\n",
    "class SkillPrior(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MeanNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "class StandardDeviationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softplus(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eff05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oposm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
