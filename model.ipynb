{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edb0f36",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Model-Based Temporal Abstraction involves simultaneuously learning\n",
    "1) skill-conditioned low-level policy\n",
    "2) skill-conditioned temporally abstract world model\n",
    "\n",
    "Notation\n",
    "- skill-conditioned low-level policy: $\\pi_{\\theta}(a_t|s_t, z)$\n",
    "    - $\\theta$ are parameters\n",
    "    - $a_t \\in A$ is current action selected by agent\n",
    "    - $s_t \\in S$ is current state\n",
    "    - $z \\in Z$ is abstract skill variable that encodes a skill\n",
    "\n",
    "- skill-conditioned temporally abstract world model (TAWM): $p_{\\psi}(s'|s,z)$ (models distribution of states agent is in after skill $z$)\n",
    "    - $\\psi$ parameters\n",
    "    - $z$ is current skill\n",
    "\n",
    "Note: low-level policy and TAWM not trained on rewards, reward function is provided later for planning with the learned skills \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c3e4c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchrl.modules import CEMPlanner\n",
    "from torch.distributions import Normal, Independent, kl_divergence\n",
    "import numpy as np\n",
    "import minari\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4bb13",
   "metadata": {},
   "source": [
    "### Learning $\\pi_{\\theta}$ and $p_{\\psi}$\n",
    "\n",
    "Learning $\\pi_{\\theta}$ and $p_{\\psi}$ requires treating skills as latent variables and optimizing the ELBO\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\psi,\\phi,\\omega)\n",
    "= \\mathbb{E}_{\\tau_T \\sim \\mathcal{D}}\\!\\left[\n",
    "  \\mathbb{E}_{q_\\phi(z\\,|\\,\\tau_T)}\\!\\left[\n",
    "    \\log \\pi_\\theta(\\bar{a}\\,|\\,\\bar{s}, z)\n",
    "    + \\log p_\\psi(s_T \\,|\\, s_0, z)\n",
    "  \\right]\n",
    "  - D_{\\mathrm{KL}}\\!\\left(q_\\phi(z\\,|\\,\\tau_T)\\,\\|\\,p_\\omega(z\\,|\\,s_0)\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "where $\\tau_T$ is a T-length subtrajectory sampled from the offline dataset $\\mathcal{D}$, $\\bar{s}$ and $\\bar{a}$ are state and action sequences of $\\tau_T$, $q_{\\psi}$ is a posterior over $z$ given $\\tau_T$, and $p_{\\omega}$ is a prior of $z$ given $s_0$.\n",
    "\n",
    "The first term is the log-likelihood of demonstrator actions. This ensures that the low-level policy can reproduce a demonstrator's action sequence given a skill. This forces $z$ to encode control-relevant information.\n",
    "\n",
    "The second term is the log-likelihood of long-term state transitions. This term ensures that we learn relationships between $z$ to what possible $s_T$ could result from. the skill.\n",
    "\n",
    "Finally, the last term is the KL divergence between skill posterior and prior (encourages compression of skills). Therefore, maximizing this ELBO makes skills $z$ explain the data and keeps the KL divergence small. This ensures that the skill is start-state predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "695e8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NEURONS = 256\n",
    "Z_DIM = 256\n",
    "\n",
    "# Skill Posterior, q_phi\n",
    "class SkillPosterior(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bi_gru = nn.GRU(input_size=NUM_NEURONS+self.action_dim, hidden_size= NUM_NEURONS//2, bidirectional=True, batch_first=True)\n",
    "        self.mean = MeanNetwork(Z_DIM)\n",
    "        self.std = StandardDeviationNetwork(Z_DIM)\n",
    "\n",
    "    def forward(self, state_sequence, action_sequence):\n",
    "        embedded_states = self.relu(self.fc1(state_sequence))\n",
    "        concatenated = torch.cat([embedded_states, action_sequence], dim=-1)\n",
    "        x, _ = self.bi_gru(concatenated)\n",
    "        mean = self.mean.forward(x)\n",
    "        std = self.std.forward(x)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "# Low-Level Skill-Conditioned Policy, pi_theta\n",
    "class SkillPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(self.action_dim)\n",
    "        self.std = StandardDeviationNetwork(self.action_dim)\n",
    "    \n",
    "    def forward(self, state, z):\n",
    "        c = torch.cat([state, z], dim=-1)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "        \n",
    "\n",
    "# Temporally-Abstract World Model, p_psi\n",
    "class TAWM(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(self.state_dim)\n",
    "        self.std = StandardDeviationNetwork(self.state_dim)\n",
    "    \n",
    "    def forward(self, input_state, z):\n",
    "        c = torch.cat([input_state, z], dim=-1)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "\n",
    "# Skill Prior, p_omega\n",
    "class SkillPrior(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(Z_DIM)\n",
    "        self.std = StandardDeviationNetwork(Z_DIM)\n",
    "    \n",
    "    def forward(self, input_state):\n",
    "        x = self.relu(self.fc1(input_state))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "\n",
    "class MeanNetwork(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "class StandardDeviationNetwork(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softplus(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4f8a0",
   "metadata": {},
   "source": [
    "#### The Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "Since calculating the true posterior of $z$ given $\\tau_T$ is intractable, we infer $q_{\\psi}(z|\\tau_T)$.\n",
    "\n",
    "1. E-Step:\n",
    "- Update $\\psi$ w/gradient descent so that KL divergence between $q_\\psi$ and true posterior is minimized\n",
    "\n",
    "2. M-Step:\n",
    "- Fixing $q_{\\psi}$, update ($\\theta, \\psi, \\omega$) s.t. ELBO is maximized using gradient ascent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c61868",
   "metadata": {},
   "source": [
    "##### E-Step (Update $\\psi$)\n",
    "\n",
    "In this step, we want to minimize \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{T}_T\\sim\\mathcal{D}}\n",
    "\\Bigg[\n",
    "\\mathbb{E}_{z\\sim q_\\phi}\n",
    "\\bigg[\n",
    "\\log \\frac{q_\\phi\\!\\left(z\\mid \\bar{s},\\bar{a}\\right)}\n",
    "{\\pi_\\theta\\!\\left(\\bar{a}\\mid \\bar{s}, z\\right)\\,p_\\omega\\!\\left(z\\mid s_0\\right)}\n",
    "\\bigg]\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "Equivalently, we want to minimize $\\mathcal{KL}(q_{\\psi}||p)$ where $p$, the true posterior is \n",
    "\n",
    "$$\n",
    "p(z \\mid \\bar{s}, \\bar{a}) = \\frac{1}{\\eta}\\,\\pi_\\theta(\\bar{a}\\mid \\bar{s}, z)\\,p_\\omega(z\\mid s_0).\n",
    "$$\n",
    "\n",
    "\n",
    "##### M-Step (Update $\\psi$)\n",
    "\n",
    "In this step, we want to update $\\theta$, $\\psi$, and $\\omega$ using gradient ascent to maximize the ELBO from above.\n",
    "\n",
    "Both steps are trained using an Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae831ff",
   "metadata": {},
   "source": [
    "##### Dataset 1: AntMaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c4a079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_maze_dataset = minari.load_dataset('D4RL/antmaze/medium-play-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1cd992f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "dict_keys(['achieved_goal', 'desired_goal', 'observation'])\n",
      "(1001, 27)\n"
     ]
    }
   ],
   "source": [
    "print(ant_maze_dataset[0].actions.shape)\n",
    "print(ant_maze_dataset[0].observations.keys())\n",
    "print(ant_maze_dataset[0].observations[\"observation\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "93eff05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, the number of subtrajectories per batch\n",
    "B = 100\n",
    "\n",
    "# T, the length of each subtrajectory\n",
    "T = 10\n",
    "\n",
    "# AntMaze state and action dims\n",
    "state_dim = 27\n",
    "action_dim = 8\n",
    "\n",
    "q_phi = SkillPosterior(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "pi_theta = SkillPolicy(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "p_psi = TAWM(state_dim=state_dim).to(device)\n",
    "p_omega = SkillPrior(state_dim=state_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "62bd4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E loss\n",
    "def compute_e_loss(subtrajectory_batch):\n",
    "    batch_s0, batch_states, batch_actions, batch_sT = subtrajectory_batch[\"s0\"], subtrajectory_batch[\"state_sequence\"], subtrajectory_batch[\"action_sequence\"], subtrajectory_batch[\"sT\"]\n",
    "    \n",
    "    zs = []\n",
    "    for i in range(len(batch_s0)):\n",
    "        mu, std = q_phi(batch_states[i], batch_actions[i])\n",
    "        epsilon = torch.randn_like(mu) # epsilon ~ N(0, 1)\n",
    "        z = mu + std * epsilon\n",
    "        zs.append(z)\n",
    "\n",
    "    e_loss = 0\n",
    "    for i in range(len(batch_s0)):\n",
    "        s0, states, actions, sT = batch_s0[i], batch_states[i], batch_actions[i], batch_sT[i]\n",
    "        mu_pi, std_pi = pi_theta(states, zs[i])\n",
    "        pi_dist = Independent(Normal(mu_pi, std_pi), 1)\n",
    "        log_pi_t = pi_dist.log_prob(actions)\n",
    "        log_pi_seq = log_pi_t.sum(dim=0)\n",
    "\n",
    "        mu_p_omega, std_p_omega = p_omega(s0)\n",
    "        p_omega_dist = Independent(Normal(mu_p_omega, std_p_omega), 1)\n",
    "        log_p_omega_t = p_omega_dist.log_prob(zs[i])\n",
    "        log_p_omega_seq = log_p_omega_t.sum(dim=0)\n",
    "\n",
    "        mu_q, std_q = q_phi(states, actions)\n",
    "        q_dist = Independent(Normal(mu_q, std_q), 1)\n",
    "        log_q_t = q_dist.log_prob(zs[i])\n",
    "        log_q_seq = log_q_t.sum(dim=0)  \n",
    "\n",
    "        e_loss += log_pi_seq + log_p_omega_seq - log_q_seq\n",
    "\n",
    "    return e_loss * -(1/B)\n",
    "\n",
    "    \n",
    "# M loss\n",
    "def compute_m_loss(subtrajectory_batch):\n",
    "    \n",
    "    zs = []\n",
    "    batch_s0, batch_states, batch_actions, batch_sT = (subtrajectory_batch[\"s0\"], subtrajectory_batch[\"state_sequence\"], subtrajectory_batch[\"action_sequence\"], subtrajectory_batch[\"sT\"]) \n",
    "    \n",
    "    for i in range(len(batch_s0)):\n",
    "        mu, std = q_phi(batch_states[i], batch_actions[i])\n",
    "        epsilon = torch.randn_like(mu) # epsilon ~ N(0, 1)\n",
    "        z = mu + std * epsilon\n",
    "        zs.append(z)\n",
    "\n",
    "    m_loss = 0\n",
    "    for i in range(len(batch_s0)):\n",
    "        s0, states, actions, sT = batch_s0[i], batch_states[i], batch_actions[i], batch_sT[i]\n",
    "        mu_pi, std_pi = pi_theta(states, zs[i])\n",
    "        pi_dist = Independent(Normal(mu_pi, std_pi), 1)\n",
    "        log_pi_t = pi_dist.log_prob(actions)\n",
    "        log_pi_seq = log_pi_t.sum(dim=0)\n",
    "\n",
    "        z_seq = zs[i].mean(dim=0)\n",
    "        mu_p_psi, std_p_psi = p_psi(s0, z_seq)\n",
    "        p_psi_dist = Independent(Normal(mu_p_psi, std_p_psi), 1)\n",
    "        log_p_psi_t = p_psi_dist.log_prob(sT)\n",
    "        log_p_psi_seq = log_p_psi_t\n",
    "\n",
    "        mu_p_omega, std_p_omega = p_omega(s0)\n",
    "        p_omega_dist = Independent(Normal(mu_p_omega, std_p_omega), 1)\n",
    "        log_p_omega_seq = p_omega_dist.log_prob(z_seq)\n",
    "\n",
    "        m_loss += log_pi_seq + log_p_psi_seq + log_p_omega_seq\n",
    "\n",
    "    return m_loss * -(1/B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0ede9632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ant_maze_dataset, T):\n",
    "        self.T = T\n",
    "        self.subtrajectories = []\n",
    "\n",
    "        for ep in ant_maze_dataset.iterate_episodes():\n",
    "            s = ep.observations[\"observation\"]\n",
    "            a = ep.actions\n",
    "            l = len(s)\n",
    "            if l < T + 1:\n",
    "                continue\n",
    "        \n",
    "        for t in range(0, l - T):\n",
    "            s0 = s[t]\n",
    "            state_sequence = s[t: t + T]\n",
    "            action_sequence = a[t: t + T]\n",
    "            sT = s[t + T]\n",
    "            self.subtrajectories.append((s0, state_sequence, action_sequence, sT))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subtrajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s0, state_sequence, action_sequence, sT = self.subtrajectories[idx]\n",
    "        return {\n",
    "            \"s0\": torch.as_tensor(s0, dtype=torch.float32),\n",
    "            \"state_sequence\": torch.as_tensor(state_sequence, dtype=torch.float32),\n",
    "            \"action_sequence\": torch.as_tensor(action_sequence, dtype=torch.float32),\n",
    "            \"sT\": torch.as_tensor(sT, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "def collate(batch):\n",
    "    return {\n",
    "        \"s0\": torch.stack([b[\"s0\"] for b in batch], 0),\n",
    "        \"state_sequence\": torch.stack([b[\"state_sequence\"] for b in batch], 0),\n",
    "        \"action_sequence\": torch.stack([b[\"action_sequence\"] for b in batch], 0),\n",
    "        \"sT\": torch.stack([b[\"sT\"] for b in batch], 0)\n",
    "    }\n",
    "\n",
    "dataset = TrainingDataset(ant_maze_dataset, T)\n",
    "loader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6d4ad24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skill_model_training():\n",
    "    q_phi.to(device).train()\n",
    "    pi_theta.to(device).train()\n",
    "    p_psi.to(device).train()\n",
    "    p_omega.to(device).train()\n",
    "\n",
    "    e_optimizer = optim.Adam(q_phi.parameters(), lr=5e-5)\n",
    "    m_optimizer = optim.Adam(list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), lr=5e-5)\n",
    "\n",
    "    for batch in loader:\n",
    "        for p in q_phi.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        for m in (pi_theta, p_psi, p_omega):\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad_(False)\n",
    "                \n",
    "        e_optimizer.zero_grad(set_to_none=True)\n",
    "        e_loss = compute_e_loss(batch)\n",
    "        e_loss.backward()\n",
    "        e_optimizer.step()\n",
    "\n",
    "        # freeze q_phi\n",
    "        for p in q_phi.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        for m in (pi_theta, p_psi, p_omega):\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad_(True)\n",
    "                \n",
    "        m_optimizer.zero_grad(set_to_none=True)\n",
    "        m_loss = compute_m_loss(batch)\n",
    "        m_loss.backward()\n",
    "        m_optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7f6f2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_model_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b1eabf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 40\n",
    "K = 1000\n",
    "L = 3\n",
    "N_keep = 200\n",
    "N_iters = 10\n",
    "tau = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d534cb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oposm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
