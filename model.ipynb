{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edb0f36",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Model-Based Temporal Abstraction involves simultaneuously learning\n",
    "1) skill-conditioned low-level policy\n",
    "2) skill-conditioned temporally abstract world model\n",
    "\n",
    "Notation\n",
    "- skill-conditioned low-level policy: $\\pi_{\\theta}(a_t|s_t, z)$\n",
    "    - $\\theta$ are parameters\n",
    "    - $a_t \\in A$ is current action selected by agent\n",
    "    - $s_t \\in S$ is current state\n",
    "    - $z \\in Z$ is abstract skill variable that encodes a skill\n",
    "\n",
    "- skill-conditioned temporally abstract world model (TAWM): $p_{\\psi}(s'|s,z)$ (models distribution of states agent is in after skill $z$)\n",
    "    - $\\psi$ parameters\n",
    "    - $z$ is current skill\n",
    "\n",
    "Note: low-level policy and TAWM not trained on rewards, reward function is provided later for planning with the learned skills \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "c3e4c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages for the whole script\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Independent, kl_divergence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import minari\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4bb13",
   "metadata": {},
   "source": [
    "### Learning $\\pi_{\\theta}$ and $p_{\\psi}$\n",
    "\n",
    "Learning $\\pi_{\\theta}$ and $p_{\\psi}$ requires treating skills as latent variables and optimizing the ELBO\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\psi,\\phi,\\omega)\n",
    "= \\mathbb{E}_{\\tau_T \\sim \\mathcal{D}}\\!\\left[\n",
    "  \\mathbb{E}_{q_\\phi(z\\,|\\,\\tau_T)}\\!\\left[\n",
    "    \\log \\pi_\\theta(\\bar{a}\\,|\\,\\bar{s}, z)\n",
    "    + \\log p_\\psi(s_T \\,|\\, s_0, z)\n",
    "  \\right]\n",
    "  - D_{\\mathrm{KL}}\\!\\left(q_\\phi(z\\,|\\,\\tau_T)\\,\\|\\,p_\\omega(z\\,|\\,s_0)\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "where $\\tau_T$ is a T-length subtrajectory sampled from the offline dataset $\\mathcal{D}$, $\\bar{s}$ and $\\bar{a}$ are state and action sequences of $\\tau_T$, $q_{\\psi}$ is a posterior over $z$ given $\\tau_T$, and $p_{\\omega}$ is a prior of $z$ given $s_0$.\n",
    "\n",
    "The first term is the log-likelihood of demonstrator actions. This ensures that the low-level policy can reproduce a demonstrator's action sequence given a skill. This forces $z$ to encode control-relevant information.\n",
    "\n",
    "The second term is the log-likelihood of long-term state transitions. This term ensures that we learn relationships between $z$ to what possible $s_T$ could result from. the skill.\n",
    "\n",
    "Finally, the last term is the KL divergence between skill posterior and prior (encourages compression of skills). Therefore, maximizing this ELBO makes skills $z$ explain the data and keeps the KL divergence small. This ensures that the skill is start-state predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the paper, each layer contains 256 neurons\n",
    "NUM_NEURONS = 256\n",
    "# The dimension of the abstract skill variable, z\n",
    "Z_DIM = 256\n",
    "\n",
    "# Skill Posterior, q_phi\n",
    "class SkillPosterior(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: sequence of skills and actions\n",
    "    Output: mean and std over z\n",
    "\n",
    "    1. Linear layer w/ ReLU activation for the state sequence\n",
    "    2. Single-layer bidirectional GRU for embedded states and action sequence (concatenated)\n",
    "    3. Extract mean and std of layer 2's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bi_gru = nn.GRU(input_size=NUM_NEURONS+self.action_dim, hidden_size= NUM_NEURONS//2, bidirectional=True, batch_first=True)\n",
    "        self.mean = MeanNetwork(Z_DIM)\n",
    "        self.std = StandardDeviationNetwork(Z_DIM)\n",
    "\n",
    "    def forward(self, state_sequence, action_sequence):\n",
    "        embedded_states = self.relu(self.fc1(state_sequence))\n",
    "        concatenated = torch.cat([embedded_states, action_sequence], dim=-1)\n",
    "        x, _ = self.bi_gru(concatenated) # [B, T, NUM_NEURONS]\n",
    "        seq_emb = x.mean(dim=1) # [B, NUM_NEURONS]\n",
    "        mean = self.mean.forward(seq_emb)\n",
    "        std = self.std.forward(seq_emb)\n",
    "        return mean, std\n",
    "\n",
    "# Low-Level Skill-Conditioned Policy, pi_theta\n",
    "class SkillPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: Current state and a skill, z\n",
    "    Output: mean and std over a\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for the state and abstract skill (concatenated)\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(self.action_dim)\n",
    "        self.std = StandardDeviationNetwork(self.action_dim)\n",
    "    \n",
    "    def forward(self, state, z):\n",
    "        c = torch.cat([state, z], dim=-1)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "        \n",
    "\n",
    "# Temporally-Abstract World Model, p_psi\n",
    "class TAWM(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: initial state, along with the abstract skill\n",
    "    Output: mean and std over terminal state\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for initial state and abstract skill (concatenated)\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim+Z_DIM, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(self.state_dim)\n",
    "        self.std = StandardDeviationNetwork(self.state_dim)\n",
    "    \n",
    "    def forward(self, input_state, z):\n",
    "        c = torch.cat([input_state, z], dim=-1)\n",
    "        x = self.relu(self.fc1(c))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "\n",
    "# Skill Prior, p_omega\n",
    "class SkillPrior(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: Initial state, s0, in the trajectory\n",
    "    Output: mean and std over the abstract skill, z\n",
    "\n",
    "    1. 2-layer shared network w/ ReLU activations for the initial state\n",
    "    2. Extract mean and std of layer 1's output\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.fc1 = nn.Linear(in_features=self.state_dim, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mean = MeanNetwork(Z_DIM)\n",
    "        self.std = StandardDeviationNetwork(Z_DIM)\n",
    "    \n",
    "    def forward(self, input_state):\n",
    "        x = self.relu(self.fc1(input_state))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x)\n",
    "        return mean, std\n",
    "\n",
    "class MeanNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: tensor to calculate mean\n",
    "    Output: mean of input w/ dimension out_dim\n",
    "\n",
    "    1. 2-layer network w/ ReLU activation for the first layer\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=NUM_NEURONS, out_features=NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(in_features=NUM_NEURONS, out_features=out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class StandardDeviationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: tensor to calculate std\n",
    "    Output: std of input w/ dimension out_dim\n",
    "\n",
    "    Note: the standard deviation is lower and upper bounded at 0.05 and 2.0\n",
    "    - if std is 0, then log(std) -> inf\n",
    "    - if std is large, then can affect training\n",
    "\n",
    "    1. 2-layer linear network with ReLU activation after first layer and softplus after second\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim, min_std=0.05, max_std=2.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(NUM_NEURONS, NUM_NEURONS)\n",
    "        self.fc2 = nn.Linear(NUM_NEURONS, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.min_std = min_std\n",
    "        self.max_std = max_std\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        std = self.softplus(x) + self.min_std  # lower bound\n",
    "        std = torch.clamp(std, max=self.max_std)\n",
    "        return std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4f8a0",
   "metadata": {},
   "source": [
    "#### The Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "Since calculating the true posterior of $z$ given $\\tau_T$ is intractable, we infer $q_{\\psi}(z|\\tau_T)$.\n",
    "\n",
    "1. E-Step:\n",
    "- Update $\\psi$ w/gradient descent so that KL divergence between $q_\\psi$ and true posterior is minimized\n",
    "\n",
    "2. M-Step:\n",
    "- Fixing $q_{\\psi}$, update ($\\theta, \\psi, \\omega$) s.t. ELBO is maximized using gradient ascent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c61868",
   "metadata": {},
   "source": [
    "##### E-Step (Update $\\psi$)\n",
    "\n",
    "In this step, we want to minimize \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\mathcal{T}_T\\sim\\mathcal{D}}\n",
    "\\Bigg[\n",
    "\\mathbb{E}_{z\\sim q_\\phi}\n",
    "\\bigg[\n",
    "\\log \\frac{q_\\phi\\!\\left(z\\mid \\bar{s},\\bar{a}\\right)}\n",
    "{\\pi_\\theta\\!\\left(\\bar{a}\\mid \\bar{s}, z\\right)\\,p_\\omega\\!\\left(z\\mid s_0\\right)}\n",
    "\\bigg]\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "Equivalently, we want to minimize $\\mathcal{KL}(q_{\\psi}||p)$ where $p$, the true posterior is \n",
    "\n",
    "$$\n",
    "p(z \\mid \\bar{s}, \\bar{a}) = \\frac{1}{\\eta}\\,\\pi_\\theta(\\bar{a}\\mid \\bar{s}, z)\\,p_\\omega(z\\mid s_0).\n",
    "$$\n",
    "\n",
    "\n",
    "##### M-Step (Update $\\theta$, $\\psi$, $\\omega$)\n",
    "\n",
    "In this step, we want to update $\\theta$, $\\psi$, and $\\omega$ using gradient ascent to maximize the ELBO from above.\n",
    "\n",
    "Both steps are trained using an Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae831ff",
   "metadata": {},
   "source": [
    "##### Dataset 1: AntMaze Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "c4a079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the AntMaze dataset in Minari format\n",
    "ant_maze_dataset = minari.load_dataset('D4RL/antmaze/medium-play-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "1cd992f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "dict_keys(['achieved_goal', 'desired_goal', 'observation'])\n",
      "(1001, 27)\n"
     ]
    }
   ],
   "source": [
    "print(ant_maze_dataset[0].actions.shape)\n",
    "print(ant_maze_dataset[0].observations.keys())\n",
    "print(ant_maze_dataset[0].observations[\"observation\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "93eff05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, the number of subtrajectories per batch (from paper)\n",
    "B = 100\n",
    "\n",
    "# T, the length of each subtrajectory (from paper)\n",
    "T = 9\n",
    "\n",
    "# AntMaze state and action dims (from Minari)\n",
    "state_dim = 27\n",
    "action_dim = 8\n",
    "\n",
    "# Initialize the models\n",
    "q_phi = SkillPosterior(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "pi_theta = SkillPolicy(state_dim=state_dim, action_dim=action_dim).to(device)\n",
    "p_psi = TAWM(state_dim=state_dim).to(device)\n",
    "p_omega = SkillPrior(state_dim=state_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d3c71a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Input: LoadedMinari dataset and the length of the number of actions in each subtrajectory (T)\n",
    "    Output: Dictionary with keys \"s0, state_sequence, action_sequence, and sT\"\n",
    "\n",
    "    Finds all episodes that have at least T actions. Then, for each of those episodes, creates a sliding window to create subtrajectories w/ T actions\n",
    "    \"\"\"\n",
    "    def __init__(self, ant_maze_dataset, T):\n",
    "        self.T = T\n",
    "        self.subtrajectories = []\n",
    "\n",
    "        for ep in ant_maze_dataset.iterate_episodes():\n",
    "            s = ep.observations[\"observation\"]\n",
    "            a = ep.actions\n",
    "            l = len(s)\n",
    "            if l < T + 1:\n",
    "                continue\n",
    "            \n",
    "            # Consider skipping timesteps so that subtrajectories don't overlap that much\n",
    "            for t in range(0, l - T):\n",
    "                s0 = s[t]\n",
    "                state_sequence = s[t: t + T]\n",
    "                action_sequence = a[t: t + T]\n",
    "                sT = s[t + T]\n",
    "                self.subtrajectories.append((s0, state_sequence, action_sequence, sT))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subtrajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s0, state_sequence, action_sequence, sT = self.subtrajectories[idx]\n",
    "        return {\n",
    "            \"s0\": torch.as_tensor(s0, dtype=torch.float32),\n",
    "            \"state_sequence\": torch.as_tensor(state_sequence, dtype=torch.float32),\n",
    "            \"action_sequence\": torch.as_tensor(action_sequence, dtype=torch.float32),\n",
    "            \"sT\": torch.as_tensor(sT, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "def collate(batch):\n",
    "    # Vertically stacks each of the components of the subtrajectories such that the first dimension is the batch\n",
    "    return {\n",
    "        \"s0\": torch.stack([b[\"s0\"] for b in batch], 0),\n",
    "        \"state_sequence\": torch.stack([b[\"state_sequence\"] for b in batch], 0),\n",
    "        \"action_sequence\": torch.stack([b[\"action_sequence\"] for b in batch], 0),\n",
    "        \"sT\": torch.stack([b[\"sT\"] for b in batch], 0)\n",
    "    }\n",
    "\n",
    "# Create the dictionary of subtrajectories\n",
    "dataset = TrainingDataset(ant_maze_dataset, T)\n",
    "\n",
    "# Iterator w/ groups of subtrajectories from the dataset of size B \n",
    "loader = DataLoader(dataset, batch_size=B, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcad857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_e_loss(batch):\n",
    "    s0  = batch[\"s0\"]               \n",
    "    S   = batch[\"state_sequence\"]   \n",
    "    A   = batch[\"action_sequence\"]  \n",
    "    Bsz, T, _ = S.shape\n",
    "\n",
    "    # Sampling z using the reparameterization trick where z = mu(tau) + std(tau) * epsilon\n",
    "    mu_q, std_q = q_phi(S, A) # [B, z_dim]\n",
    "    eps = torch.randn_like(mu_q)\n",
    "    z = (mu_q + std_q * eps)      \n",
    "\n",
    "    # Freeze the weights of the low-level skill-conditioned policy, pi_theta\n",
    "    with torch.no_grad():\n",
    "        z_bt = z.unsqueeze(1).expand(Bsz, T, -1) # [B, T, z_dim]\n",
    "        # Flatten time dimension since pi_theta does not the expect extra dim ([B * T, s_dim])\n",
    "        Sf = S.reshape(Bsz*T, -1)\n",
    "        Zf = z_bt.reshape(Bsz*T, -1)\n",
    "        mu_pi, std_pi = pi_theta(Sf, Zf)     \n",
    "        # Restore time dimension ([B, T, a_dim])\n",
    "        mu_pi  = mu_pi.view(Bsz, T, -1)\n",
    "        std_pi = std_pi.view(Bsz, T, -1)\n",
    "        # Build a MVN over independent actions at each timestep of each batch and sum log_probs across action_dim\n",
    "        pi_dist = Independent(Normal(mu_pi, std_pi), 1)\n",
    "        # Compute the log probability of observed actions\n",
    "        log_pi = pi_dist.log_prob(A) # ([B, T])    \n",
    "\n",
    "    # Freeze the weights of the skill prior, p_omega\n",
    "    with torch.no_grad():\n",
    "        # Find the distribution of the abstract skill given start states (mu_pr & std_pr: [B, z_dim])\n",
    "        mu_pr, std_pr = p_omega(s0)  \n",
    "        # Build a MVN over independent skills at each timestep of each batch and sum log_probs across z_dim\n",
    "        prior_dist = Independent(Normal(mu_pr, std_pr), 1)\n",
    "        # Compute the log-probability over the sampled skills using the prior\n",
    "        log_p_omega = prior_dist.log_prob(z)   \n",
    "\n",
    "    post_dist = Independent(Normal(mu_q, std_q), 1)\n",
    "    # Compute the log-probability over the sampled skills using the inferred posterior\n",
    "    log_q = post_dist.log_prob(z)              \n",
    "\n",
    "    # Calculate the E-objective\n",
    "    e_obj = (log_pi.sum(dim=1) + log_p_omega - log_q)\n",
    "    e_loss = -e_obj.mean() # minimize the negative objective\n",
    "    return e_loss\n",
    "\n",
    "\n",
    "\n",
    "def compute_m_loss(batch):\n",
    "    s0  = batch[\"s0\"]               \n",
    "    S   = batch[\"state_sequence\"]   \n",
    "    A   = batch[\"action_sequence\"] \n",
    "    sT  = batch[\"sT\"]               \n",
    "    Bsz, T, Sdim = S.shape\n",
    "\n",
    "    # Freeze the weights of the inferred posterior, only update omega, psi, and theta\n",
    "    with torch.no_grad():\n",
    "        mu_q, std_q = q_phi(S, A)        # [B, z_dim]\n",
    "        eps = torch.randn_like(mu_q)\n",
    "        # same reparameterization trick as E-loss\n",
    "        z = (mu_q + std_q * eps)  \n",
    "\n",
    "    z_bt = z.unsqueeze(1).expand(Bsz, T, -1)\n",
    "    # Flatten time dimension since pi_theta does not the expect extra dim ([B * T, s_dim])\n",
    "    Sf = S.reshape(Bsz*T, -1)\n",
    "    Zf = z_bt.reshape(Bsz*T, -1)\n",
    "    mu_pi, std_pi = pi_theta(Sf, Zf)    \n",
    "    # Restore time dimension ([B, T, a_dim])     \n",
    "    mu_pi = mu_pi.reshape(Bsz, T, -1)\n",
    "    std_pi = std_pi.reshape(Bsz, T, -1)\n",
    "    # Build a MVN over independent actions at each timestep of each batch and sum log_probs across action_dim\n",
    "    pi_dist = Independent(Normal(mu_pi, std_pi), 1)\n",
    "    log_pi = pi_dist.log_prob(A)               \n",
    "\n",
    "    # Zs: [B, T, z_dim]\n",
    "    mu_T, std_T = p_psi(s0, z)\n",
    "    # Build a MVN over independent terminal states at each timestep of each batch and sum log_probs across state_dim     \n",
    "    ppsi_dist = Independent(Normal(mu_T, std_T), 1)\n",
    "    # Compute the log-probability over the observed terminal states using the TAWM\n",
    "    log_p_psi = ppsi_dist.log_prob(sT)          \n",
    "\n",
    "    # Find the distribution of the abstract skill given start states (mu_pr & std_pr: [B, z_dim])\n",
    "    mu_pr, std_pr = p_omega(s0)                \n",
    "    # Build a MVN over independent skills at each timestep of each batch and sum log_probs across z_dim\n",
    "    prior_dist = Independent(Normal(mu_pr, std_pr), 1)\n",
    "    # Compute the log-probability over the sampled skills using the prior\n",
    "    log_p_omega = prior_dist.log_prob(z)   \n",
    "\n",
    "    # sum to find join log-likelihood of the whole sequence\n",
    "    obj = log_pi.sum(dim=1) + log_p_psi + log_p_omega   \n",
    "    m_loss = -obj.mean()\n",
    "    return m_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "6d4ad24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def skill_model_training():\n",
    "    q_phi.to(device).train()\n",
    "    pi_theta.to(device).train()\n",
    "    p_psi.to(device).train()\n",
    "    p_omega.to(device).train()\n",
    "\n",
    "    e_optimizer = optim.Adam(q_phi.parameters(), lr=5e-5)\n",
    "    m_optimizer = optim.Adam(list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), lr=5e-5)\n",
    "\n",
    "    for batch in loader:\n",
    "        for p in q_phi.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        for m in (pi_theta, p_psi, p_omega):\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad_(False)\n",
    "                \n",
    "        e_optimizer.zero_grad(set_to_none=True)\n",
    "        e_loss = compute_e_loss(batch)\n",
    "        e_loss.backward()\n",
    "        e_optimizer.step()\n",
    "\n",
    "        # freeze q_phi\n",
    "        for p in q_phi.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        for m in (pi_theta, p_psi, p_omega):\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad_(True)\n",
    "                \n",
    "        m_optimizer.zero_grad(set_to_none=True)\n",
    "        m_loss = compute_m_loss(batch)\n",
    "        m_loss.backward()\n",
    "        m_optimizer.step()\n",
    "\n",
    "        print(f\"E-Loss = {e_loss}\")\n",
    "        print(f\"M-Loss = {m_loss}\")\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def skill_model_training(\n",
    "    loader,\n",
    "    q_phi, pi_theta, p_psi, p_omega,\n",
    "    e_lr=5e-5, m_lr=5e-5,\n",
    "    epochs=50,\n",
    "    e_steps=1, m_steps=1,\n",
    "    grad_clip=1.0 # prevents runaway gradients\n",
    "):\n",
    "    # Skill model training setup\n",
    "    q_phi.to(device)\n",
    "    pi_theta.to(device)\n",
    "    p_psi.to(device)\n",
    "    p_omega.to(device)\n",
    "\n",
    "    e_optimizer = torch.optim.Adam(q_phi.parameters(), lr=e_lr)\n",
    "    m_optimizer = torch.optim.Adam(list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), lr=m_lr)\n",
    "\n",
    "    e_curve = []   \n",
    "    m_curve = []   \n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Running e_loss, m_loss, and batches in current epoch\n",
    "        e_running, m_running, nb = 0.0, 0.0, 0\n",
    "\n",
    "        for batch in loader:\n",
    "            # Rebuilds dictionary but moves tensors to the device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            nb += 1\n",
    "\n",
    "            # In E-step, train the posterior while freezing other parameters\n",
    "            q_phi.train()\n",
    "            pi_theta.eval()\n",
    "            p_psi.eval()\n",
    "            p_omega.eval()\n",
    "\n",
    "            for p in q_phi.parameters(): \n",
    "                p.requires_grad_(True)\n",
    "            for m in (pi_theta, p_psi, p_omega):\n",
    "                for p in m.parameters(): \n",
    "                    p.requires_grad_(False)\n",
    "\n",
    "            # For the e-step, \n",
    "            for _ in range(e_steps):\n",
    "                # Resent gradients\n",
    "                e_optimizer.zero_grad(set_to_none=True)\n",
    "                e_loss = compute_e_loss(batch)\n",
    "                e_loss.backward() \n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(q_phi.parameters(), grad_clip)\n",
    "                e_optimizer.step() # Update the parameters of the posterior\n",
    "\n",
    "            e_running += e_loss.item()\n",
    "\n",
    "            # Freeze posterior weights, update all other weights\n",
    "            q_phi.eval()\n",
    "            pi_theta.train()\n",
    "            p_psi.train()\n",
    "            p_omega.train()\n",
    "\n",
    "            for p in q_phi.parameters(): \n",
    "                p.requires_grad_(False)\n",
    "            for m in (pi_theta, p_psi, p_omega):\n",
    "                for p in m.parameters(): \n",
    "                    p.requires_grad_(True)\n",
    "\n",
    "            for _ in range(m_steps):\n",
    "                # Reset gradients\n",
    "                m_optimizer.zero_grad(set_to_none=True)\n",
    "                m_loss = compute_m_loss(batch)\n",
    "                m_loss.backward()\n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(list(pi_theta.parameters()) + list(p_psi.parameters()) + list(p_omega.parameters()), grad_clip)\n",
    "                m_optimizer.step() # Update theta, psi, and omega\n",
    "\n",
    "            m_running += m_loss.item()\n",
    "\n",
    "        # Calculate the average losses over all the batches in the epoch\n",
    "        e_epoch = e_running / max(1, nb)\n",
    "        m_epoch = m_running / max(1, nb)\n",
    "        e_curve.append(e_epoch)\n",
    "        m_curve.append(m_epoch)\n",
    "        print(f\"[Epoch {epoch:03d}/{epochs}]  E: {e_epoch:.4f}   M: {m_epoch:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(e_curve, label=\"E-loss\")\n",
    "    plt.plot(m_curve, label=\"M-loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"EM training losses\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return e_curve, m_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f2da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001/50]  E: 34.6321   M: 587.2893\n",
      "[Epoch 002/50]  E: 18.7181   M: 564.2154\n",
      "[Epoch 003/50]  E: 14.3338   M: 555.5850\n",
      "[Epoch 004/50]  E: 11.8708   M: 551.0396\n",
      "[Epoch 005/50]  E: 10.2931   M: 548.5038\n",
      "[Epoch 006/50]  E: 9.2057   M: 546.7163\n",
      "[Epoch 007/50]  E: 8.4040   M: 545.4306\n",
      "[Epoch 008/50]  E: 7.7825   M: 544.3742\n",
      "[Epoch 009/50]  E: 7.2769   M: 543.5378\n",
      "[Epoch 010/50]  E: 6.8484   M: 542.9135\n",
      "[Epoch 011/50]  E: 6.4756   M: 542.3097\n",
      "[Epoch 012/50]  E: 6.1471   M: 541.7917\n",
      "[Epoch 013/50]  E: 5.8573   M: 541.3920\n",
      "[Epoch 014/50]  E: 5.6009   M: 540.9686\n",
      "[Epoch 015/50]  E: 5.3650   M: 540.6116\n",
      "[Epoch 016/50]  E: 5.1603   M: 540.2273\n",
      "[Epoch 017/50]  E: 4.9698   M: 539.9982\n",
      "[Epoch 018/50]  E: 4.7942   M: 539.7353\n",
      "[Epoch 019/50]  E: 4.6350   M: 539.4810\n",
      "[Epoch 020/50]  E: 4.4839   M: 539.2131\n",
      "[Epoch 021/50]  E: 4.3414   M: 538.9758\n",
      "[Epoch 022/50]  E: 4.2132   M: 538.7621\n",
      "[Epoch 023/50]  E: 4.0907   M: 538.6684\n",
      "[Epoch 024/50]  E: 3.9821   M: 538.4522\n"
     ]
    }
   ],
   "source": [
    "skill_model_training(loader, q_phi, pi_theta, p_psi, p_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eabf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 40\n",
    "K = 1000\n",
    "L = 3\n",
    "N_keep = 200\n",
    "N_iters = 10\n",
    "tau = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87568757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oposm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
